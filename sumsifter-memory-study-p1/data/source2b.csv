id,text
S1,<h2>SCADS 2024 Problem Book  (S1).</h2>
S2,"<p>Violet B., Susanna Bitters, John M (S2). Conroy, R (S3). Jordan Crouser, Sue Mi Kim, J (S4). Bowman Light, Neil P (S5). Molino, Amanda Peterson, Elizabeth Richerson, Stephen Shauger, Ben Strickson, Aaron W., Julia S (S6). Yang (S7).</p>"
S3,<p>April 2024  (S8).</p>
S4,<h2>1 Introduction  (S9).</h2>
S5,"<p>The Summer Conference on Applied Data Science (SCADS) is an annual eight-week summer research program held at the Laboratory for Analytic Sciences (LAS) that is focused on the research and development of Artificial Intelligence (AI) to assist US Intelligence Community (IC) data analysts (S10). SCADS identifies a multi-year Grand Challenge to focus the research toward solving a current mission challenge (S11).  SCADS brings together approximately fifty personnel each year, including faculty, graduate students, industry professionals, researchers at National Laboratories and Federally Funded Research and Development Centers (FFRDCs), and government employees from the IC, to advance the research on the current Grand Challenge. (S12).</p>"
S6,<p>Grand Challenge: Generate tailored daily reports for knowledge workers that capture information relevant to their individual objectives and interests (S13).</p>
S7,"<p>The SCADS grand challenge is intended to be a multi-year unifying research goal of creating Tailored Daily Reports (TLDRs) for individual knowledge workers within the IC (S14). These reports would be similar in some sense to the well-known President’s Daily Brief or to established commercial news aggregators, but would include a mix of classified and unclassified material, combine information from a variety of modes and formats, and be tailored to the interests and responsibilities of the individual IC worker (S15).</p>"
S8,"<p>In 2024, we continue working toward creating TLDRs and directed research efforts will fall under the broad areas of Automatic Summarization, Recommendation Systems, and Human-Machine Interaction (S16). Each of these areas will also include a special emphasis on explainability and contextualization, as well as knowledge representation and dataset creation and curation (S17). Efforts will also include development projects that are aimed at bringing together the different areas into a demonstration application to model how the individual research projects come together to support a TLDR (S18).</p>"
S9,"<p>In this document we present relevant critical challenges and research questions that address various aspects of the grand challenge, briefly discuss the motivation for those challenges and questions, and provide references and other resources for further exploration (S19). We have organized this document by focus areas, and note that some challenges and questions presented might span multiple focus areas (S20). For each critical challenge and research question, we attempt to identify the scope (short, medium, or long-term) of the challenge or question, and also which other challenges and questions are related (S21).</p>"
S10,"<p>Throughout this document we attempt to identify research areas which may be served by use of Large Language Models (LLMs) which have been an area of great focus in the field recently (S22). Two ubiquitous LLMs are GPT-3.5 and GPT-4, versions of which are incorporated into ChatGPT (S23).</p>"
S12,<p>2 Automatic Summarization  (S24).</p>
S13,"<p>Key to the creation of a TLDR will be the automatic summarization of large corpora of documents (S25). This process will involve both extractive and abstractive summarization (S26). While summarization for SCADS is primarily concerned with text, research in multi-modal summarization techniques is also encouraged (S27). Researchers would have access to existing summarization engines and be able to improve and augment them (S28). Traditionally, automatic summarization has fallen into two categories: extractive and abstractive (S29).</p>"
S14,"<p>Extractive summarization is the process of extracting key sub-portions of a document, typically at the sentence level, and compiling those sub-portions into a readable and salient summary of the document (S30). The main extractive summarization package we will use at SCADS is occams [1] (S31). The occams package employs an optimization-based extractive algorithm (S32). This family of methods is known to perform best among extractive systems on a wide range of evaluation data (S33). For a survey of the summarization problem and classical extractive summarization methods, we recommend [2] (S34). Also, see [3] for an overview with a description of some successful neural-based abstractive summarizers (S35).</p>"
S15,"<p>Abstractive summarization is the process of generating novel prose providing a readable and salient summary of a document based on its content, rather than extracting sub-portions of the document as in extractive summarization (S36). Large Language Models (LLMs) employ abstractive summarization (S37). For SCADS, we will focus on models made available by Hugging Face, the OpenAI API, and other models as applicable (S38). Open-source systems available via Hugging Face are recent small to midsize neural systems using variations of the transformer language model, the first of which was BERT [4] (S39). Models of this size serve as practical examples of abstractive summarization capabilities in a computing environment similar to what might be expected for a TLDR prototype or operational system (S40). Other larger language models, such as GPT-3 and -4, are also eligible for inclusion during SCADS (S41). The focus on such large language models might shift to address how such models might be practically incorporated into a TLDR scenario, for example, through exploration of hybrid implementations with an extractive summarizer, careful prompt creation, or Retrieval Augmented Generation (RAG) [5] (S42).</p>"
S16,"<p>For all automatically created summaries, it will be important to provide context for where the information contained in the summary originated (S43). This might be in the form of references, citations, or other methods enabling users to move from the summary to the underlying source data (S44). See 4.4 for more discussion of providing context for content included in a TLDR (S45).</p>"
S18,"<p>Critical Challenge 2.A Develop summaries of documents personalized to a user’s interests, employing abstractive extractive, and/or hybrid methods (S46).</p>"
S19,"<p>As improvements are made in the language model space, the scope of summarization is increasing (S47). Topics in question answering, information retrieval, and entity extraction are becoming more mainstream for summarization engines within traditional summarization, there are various methods to investigate that focus on different aspects of the text (S48). A past Document Understanding Conference hosted by NIST investigated topic-based summarization (S49). Other potential areas could be guided and aspect summaries, all of which incorporate named entities. (S50).</p>"
S21,<p>2.1 Text Embedding  (S51).</p>
S22,"<p>One outgrowth of natural language processing developments is the ability to embed not only words in context but also sentences and documents (S52). Such methods have been used to develop topic models, for example, top2vec clusters documents employing such embeddings (S53). It is natural to consider methods of utilizing sentence embeddings to improve extractive summarization (S54).</p>"
S23,<p>Question 2.1a How can sentence embeddings best be used to improve extractive summarization? Scope: medium-term  (S55).</p>
S24,"<p>Countless analysis techniques can be applied to high-dimensional embeddings to offer insight into a sentence’s purpose within a document, and conceivably into its value to an extractive summary (S56). In essence, the embeddings provide a convenient way to extract some of the power of a pre-trained language model for a computational cost that, while significantly more than an extractive model like occams with NLTK segmentation, is far less than that of generating an abstractive summary with a distilled BERT model (S57).</p>"
S25,"<p>Sentences are typically used as a proxy for a single unit of information, but larger or smaller chunks can be embedded as well (S58). In situations where large amounts of text are being summarized, entire paragraphs or documents can be embedded as a single chunk, While there are costs to embedding larger pieces of text, an exploration of the optimal chunk size to embed for summarization could be beneficial (S59).</p>"
S27,<p>Question 2.1b How does adjusting the length of embedded text impact extractive summaries? Scope: medium-term  (S60).</p>
S29,"<p>While embedding models on their own can be a powerful tool in exploring connections within a document, they can also be used for information retrieval tasks (S61). A vector database is a collection of embedded data stored in multi-dimensional space (S62). This allows users to perform vector search on the data, finding the entries which are most similar to the input query (S63). This could prove beneficial when attempting to summarize certain topics or events from a large corpus of data (S64).</p>"
S30,<p>Question 2.1c What is the optimal chunk size for storing embedded documents in a vector store for later retrieval? Scope: medium-term  (S65).</p>
S32,<p>2.2 Retrieval Augmented Generation  (S66).</p>
S33,"<p>During the proceedings of SCADS 2023, LLMs showed a lot of promise for summarization tasks (S67). Multiple teams investigated abstractive summaries generated with GPT-3.5 Turbo and other models and compared the performance of these summaries on common benchmarks in the community (S68). However, a common issue noted by the community is hallucinations, or statements presented in the summary that are not present in the original text (S69). These models have been trained on vast quantities of data and tend to rely on their training data rather than solely on the summarization target (S70). Additionally, multi-document summarization can be difficult for models given context window constraints (S71). In recent months, Retrieval Augmented Generation (RAG) has become a popular model architecture to combat these concerns (S72).</p>"
S36,"<p>A standard RAG pipeline consists of an embedding model, a vector store, and an LLM (S73). Typically, this pipeline functions by providing an external data store for the LLM, which is used to create responses to user-inputted queries (S74). The way that the information is retrieved from the store can vary, but simple pipelines will just return a set number of entries with the highest vector similarity to the query in the embedding space (S75). These entries are then integrated as context within a prompt for the large language model (S76). Finally, the output from the model is returned to the user, potentially along with the sources provided as context (S77).</p>"
S38,<p>Critical Challenge 2.B How can Retrieval Augment Generation (RAG) be applied to achieve multi-document summarization and synthesis for a TLDR? Scope: long-term  (S78).</p>
S39,"<p>A common criticism of most mainstream LLM applications is the ‘black box’ nature of the responses and the high potential for hallucinations (S79). RAG addresses these concerns by having the ability to trace what context was provided to generate a response (S80). There is still the potential for answers to contain errors, but incorporating RAG into a LLM chat application has been shown to reduce the number of hallucinations generated [7] (S81). Users interacting with RAG applications can pose queries beyond summarizing a subset of documents (S82). Having a large vector store allows queries to reference a large body of text, answering questions on it and providing key summaries (S83). While RAG has a lot of potential in the space, it is still relatively novel, and research is still exploring what use cases are most suited to it (S84). Consulting with analysts and other TLDR customers to carry out some typical queries on a RAG pipeline will help identify pain points and areas for future research (S85). Analysts may also be able to provide feedback regarding the documents that are returned during the retrieval step and how that information should be incorporated into what the user sees returned from the model (S86).</p>"
S41,"<p>Question 2.2a What use cases are particularly suited or not suited for RAG, as compared to a standard instruction tuned LLM? Scope: short-term  (S87).</p>"
S43,"<p>All applications that provide open-ended queries to LLMs have the potential for prompt-tuning to ensure a desirable output from the model (S88). Different styles of prompting can yield different responses from the model, which may be more or less suited to what the user expects (S89). With a RAG pipeline, there is the added requirement of incorporating the context documents to the responses (S90). In order to save tokens and potentially get more information into the context limit, extractive summarization may prove useful when constructing the prompt for the LLM (S91).</p>"
S45,<p>Question 2.2b Can we characterize and/or measure the impact of different prompting styles in summarization systems employing large language models? Scope: medium-term  (S92).</p>
S46,"<p>Both RAG and hybrid summarization involve an intermediate step before passing documents to an abstractive summarizer, aiming to reduce hallucinations and unnecessary text (S93). While a classic extractive summarizer would pull important sentences from the source documents, a RAG pipeline typically retrieves larger chunks from the corpus (S94). Hybrid summarization methods can combine the extractive step with a retrieval step as well, before passing to an abstractive summarizer (S95). In practice, there are many ways to refine and tune both of these methods (S96). Numerous techniques and packages have been written to combine both of these approaches, including but not limited to extractive summarization (S97).</p>"
S47,<p>Question 2.2c Does a hybrid approach to RAG that incorporates extractive summarization improve the pipeline? Scope: short-term  (S98).</p>
S49,<p>2.3 Summarizing a Changing Corpus (S99).</p>
S50,"<p>The data sources that analysts rely upon to generate reporting are constantly changing (S100). As situations evolve, new facts arise, and old information is augmented at an extremely fast pace (S101). In order for a TLDR to provide utility, it must be able to adapt just as quickly as the underlying data does (S102). Taking into account when data was generated is important in ensuring that analysts are receiving the most up to date facts and are not being fed outdated information (S103). In this section we explore the established technique SAGA and propose potential methods of incorporating temporality into a RAG system (S104).</p>"
S51,<p>Critical Challenge 2.C Develop summarization methods to focus on new content in a changing corpus and provide the user with an updated summary (S105). Scope: long-term  (S106).</p>
S52,"<p>This section will introduce SAGA, a technique that extends any similarity function on elements (items) to a set (S107). Suppose we define a similarity function between two items, for example, two sentences or documents (S108). The similarity function could be the cosine score between the bag of word counts or some continuous embedding of the sentences or documents (S109). SAGA gives an algorithm to promote such a similarity score to collections of sentences or documents (S110). This SAGA score can then be used to cluster the items (S111). See Appendix B for further details on SAGA and its application (S112).</p>"
S53,"<p>SAGA has the potential to be used to focus term weights on new information (S113). The update summarization task (critical challenge 2.C) is one opportunity to employ SAGA (S114). SAGA is a very general idea and could be used for either a simple bag of words model or word, sentence, or topic embeddings (S115). SCADS will have access to C code for computing SAGA similarities as well as examples where others have used SAGA in conjunction with word embeddings (S116).</p>"
S55,<p>Question 2.3a Can SAGA be used to produce summaries which focus on new information in a temporal corpus? Scope: short-term   (S117).</p>
S56,"<p>RAG typically does not incorporate temporality into its responses by default, but with some adjustments to the vector store and retrieval methods we can favor documents that are more recent (S118). Work has been done to incorporate temporal elements into retrieval-augmented pipelines [8], notably through a re-ranker function (S119). Further investigation into methods of time-aware retrieval would be useful in ensuring that the documents being used to generate responses contain the most up to date information (S120).</p>"
S58,<p>Question 2.3b How can a RAG pipeline take into account the recency of documents when retrieving and generating responses? Scope: medium-term  (S121).</p>
S60,<p>2.4 Automatic Evaluation for Factuality and Benefit (S122).</p>
S61,<p>Critical Challenge 2.D Develop a technique to apply automatic fact-checking to abstractive summaries (S123). Scope: medium-term   (S124).</p>
S62,"<p>Abstractive summaries are known to suffer from hallucinations, or statements that are not supported by the text (S125). In some cases these statements are false and in other cases there is simply insufficient evidence in the document to support the claim in the summary (S126). Previous research has proposed many tools to fact-check based on automatic question and answering systems as well as textual entailment (S127).</p>"
S63,"<p>Early work in extractive summarization used rewrites and anaphora resolution methods to improve readability and remove ambiguity [9] (S128). Current abstractive methods generally produce a fluent text which is rarely ambiguous but may not be factual (S129). Many of these errors result from using the wrong entity, e.g., the name, organizations, date, etc (S130). Even large language models, such as those produced by OpenAI, may have an alarming error rate of 20% or more [10] (S131). Although, very recently, a group of researchers from Stanford and Columbia Universities found that a fine-tuned smaller model can produce summaries with a much lower error rate, perhaps as low as 1% to 3% [11] (S132).</p>"
S64,"<p>In Holistic Evaluation of Language Models (HELM) [12], the faithfulness of generated summaries is measured using Natural Language Inference (NLI) based SummaC [13] and QA-based QAFactEval [14] (S133). (See pages 23-24 HELM) (S134). SCADS 2023 researchers investigated SummaC for hallucination detection and deletion of extraneous summary sentences (S135). QAFactEval was also the focus of an initial study at SCADS 2022 (S136). For background references, a recent survey paper was published in the Transactions of the Association for Computational Linguistics [15], and see also the recent ACM Computing Survey on hallucination in natural language generation [16] (S137).</p>"
S66,<p>Question 2.4a How can we best adapt the tools from the HELM (Holistic Evaluation of Language Models) automatic evaluation to measure hallucination on our data at SCADS? Scope: medium-term   (S138).</p>
S68,"<p>Question 2.4b Can we employ characteristics of a data set, such as a fixed taxonomy or ontology, to support automated fact-checking of an automatically-generated summary? Scope: medium-term   (S139).</p>"
S69,"<p>RAG methods show promise for generating responses grounded in truth, as the prompt provided to the LLM is augmented with specific documents from the data store (S140). By directly providing segments of these documents to the user, we can provide more context for where summaries are drawing information from (S141). However, there may be some incorrect information provided by the LLM that misrepresents the data from the context (S142). Further investigation is needed to evaluate the potential for fact-checking and validation using these methods (S143).</p>"
S70,<p>Question 2.4c Can we use retrieval-augmented generation to fact-check abstractive or hybrid summaries? Scope: short-term (S144).</p>
S72,"<p>Another method of determining whether a summary properly reflects the documents at hand is via attribution (S145). Ideally, all sentences in a generated summary could be directly attributed to the segments of the original text being summarized (S146). During SCADS 2023, researchers conducted a human evaluation study to investigate using NLI and sentence embeddings to determine the attribution of specific summary sentences (S147). This method showed a lot of promise in determining the origin of summary sentences, and these methods could be extended to further prove whether a summary is properly attributable to the source documents (S148). Other research into attribution at SCADS 2023 demonstrated some of the potential pitfalls of attribution and demonstrated that an attribution method beyond linking the highest scoring sentences in the summary and source could yield improved results (S149). An investigation into other methods of determining attribution may yield methods that can relate specific facts and provide citations for the generated sentences in the summary (S150).</p>"
S74,<p>Question 2.4d How can attribution be used to prove a summary is grounded in source documents and does not contain hallucinations? Scope: medium-term  (S151).</p>
S76,"<p>Responses generated by RAG pipelines should also have demonstrable attribution, as context documents are used to directly enhance the context provided to the summarizer (S152). This could be carried out using the methods explored at SCADS 2023, or via prompt tuning (S153).</p>"
S77,<p>Question 2.4e How can a RAG solution leverage the returned context documents to provide justification and attribution for the resulting response? Scope: short-term  (S154).</p>
S78,<p>Related: Section 2.2  (S155).</p>
S80,<p>2.5 Human Evaluation  (S156).</p>
S81,"<p>Critical Challenge 2.E Develop an infrastructure to incorporate human evaluation of automatically generated summaries (S157). Scope: medium-term, Related: Section 4.4   (S158).</p>"
S82,"<p>Human evaluation of summaries can focus on various aspects of the summary, including content, accuracy, and readability (S159). Ideally, each of these quality aspects should be evaluated in a quantitative method (S160). Unfortunately, determining objective measures for aspects such as conciseness and interpretability can be difficult (S161). A method of combating this is through Automatic Content Units (ACUs), which can be used to identify only the most critical pieces of information in a document (S162). By ensuring that the generated summary includes these ACUs without containing excessive additional information, we can quantify how efficient the summary is without becoming difficult to interpret (S163). One such method defines a two-stage method using ACUs that provides automatic evaluation of summaries along these areas [17], with accompanying code available on Github10 (S164). However, combining this method with human evaluators would provide further insight into the quality of ACUs and the ease of interpretability of generated summaries (S165).</p>"
S83,<p>Question 2.5a How can a human-machine teaming approach be used to evaluate summaries? Scope: medium-term  (S166).</p>
S84,"<p>Next, we can investigate the content and accuracy of a summary (S167). Ideally, a summary should allow the user to learn some main facts about a document easily (S168). Given a collection of human-written summaries, the automatic systems QAFactEval could automatically generate questions and answer (Q&A) pairs (S169). These Q&A pairs could then be presented to the human evaluator to answer the yes or no question, “Does this summary correctly answer this question?” A separate evaluation of the Q&A pairs could be done to verify the accuracy of QAFactEval (S170). The Cyber Threat Intelligence (CTI) data set could be useful in experiments related to Q&A-based evaluations (S171).</p>"
S85,"<p>Question 2.5b Given a set of source documents and associated question-answer pairs, how do different summarization approaches compare on capturing relevant information in the summaries to answer the questions? Scope: short-term  (S172).</p>"
S86,"<p>An approach that looks quite viable was recently published [18] (S173). Here the authors automatically generate multiple-choice questions and answers, which are then used to evaluate the summary for content and factuality (S174). The code for this approach is available on GitHub11 (S175).</p>"
S87,"<p>Conceivably, human evaluators could also identify and classify hallucinations in abstract or hybrid summaries (S176). The aforementioned survey [16] provides more information (S177). Unfortunately, it is not a trivial evaluation task for human evaluators to read and comprehend source material, then one or more automatically generated summaries, and finally to identify and classify any hallucinations in the summaries (S178). The CTI data set could be useful for identifying hallucinations in abstractive or hybrid summaries of the source data and offer insight into the hallucination problem as it pertains to the TLDR application/user base (S179). Other suitable data sets might also exist (S180). The LAS-developed data labeling application Infinitypool could be particularly useful to facilitate necessary annotation required when conducting such an evaluation (S181).</p>"
S88,"<p>Chatbots built from Large Language Models (LLMs), such as ChatGPT, use reinforcement learning from human feedback (RLHF) to become proficient at responding to natural language prompts [19] (S182). A similar approach might be used to fine tune LLMs used for summarization (S183).</p>"
S89,<p>Question 2.5c Can we use reinforcement learning and human feedback to improve the performance of abstractive or hybrid summarization models?  (S184).</p>
S90,<p>Critical Challenge 2.F Design/execute an experiment to identify and classify hallucinations in automatically generated summaries (S185). Scope: short-term  (S186).</p>
S91,<p>2.6 Summarizing Non-Text Data (S187).</p>
S92,"<p>The eventual TLDR will need to summarize not just articles but also input from a variety of modalities (S188). Two modalities of interest are audio, and video content (S189).  Conversational Audio is an instance of the more general problem of dialogue summarization [20] [21] (S190). An audio conversation is likely to require different techniques than summarizing an article or similar document (S191). This is for a variety of reasons, including that a single transcription of an audio conversation could include many different topics (S192). It will also include more colloquial language than a prepared article (S193). Finally, there will be transcription errors due to both out-of-vocabulary words and also audio quality (S194). Work done at SCADS 2022 suggested that extractive summarization techniques may not be suitable for noisy transcripts (S195). Recent experiments at LAS with a large language model indicate slightly more promising results, though these experiments were extremely limited and results remained fairly poor, particularly on samples with poor audio quality (S196). Therefore, we propose the following questions on the topic of the summarization of audio (S197).</p>"
S93,"<p>Question 2.6a Could an abstractive or hybrid summarization system provide informative summaries of a transcription of a conversational audio file? Scope: medium-term, Related: Section 4.4   (S198).</p>"
S95,"<p>Question 2.6b What types of information should be included in a summary of an audio conversation? Scope: medium-term, Related: Section 4.4   (S199).</p>"
S97,"<p>With LLMs becoming more and more capable of handling multi-modal content, there is a lot of promise for summarizing video data (S200). Using methods to label objects in video frames, follow movement of objects, and other video processing techniques (S201). Utilizing annotations and change-tracking, knowledge graphs could be used in conjunction with LLMs to generate fluent summaries of videos for an analyst user (S202). While there are less identified datasets dedicated to video summarization as compared to text, there is a potential for utilizing open source videos or creating data at SCADS 2024 using new technology such as VideoGPT [22] (S203).</p>"
S99,<p>Question 2.6c Could an abstractive or hybrid summarization system provide informative summaries of a video? Scope: medium-term  (S204).</p>
S101,<p>Question 2.6d What datasets could we find or generate to create and evaluate video summarization methods? Scope: short-term  (S205).</p>
S102,"<p>Image data, whether from video stills, photographs, or drawings, is also a candidate for multi-modal summarization methods: (S206).</p>"
S103,"<p>Question 2.6e Can a model effectively summarize an image, such as a network diagram? Scope: medium term (S207).</p>"
S104,<p>Question 2.6f Can insights provided by graphic content be effectively summarized and delivered in natural language text? Scope: short-term (S208).</p>
S105,"<p>Question 2.6g How does the use of associated natural language text (e.g (S209). notations on a drawing, a sign in a photo) improve the extraction of insights from graphic content? Scope: short to medium-term, Related: Section 4.4 (S210).</p>"
S106,"<p>Question 2.6h Does use of multimodal summarization models, particularly graphic content alongside natural language text, improve the perceived quality of summaries for specialized knowledge workers? To what degree does the use of graphic content change the summarization? Scope: short-term (S211).</p>"
S107,"<p>Finally, while communications data such as network packets can technically be viewed as text, especially using tools like Wireshark, tracing that traffic is an intensive process (S212). Similarly, while tools such as Ghidra exist to reverse-engineer the code behind a computer executable, that code is rarely in an easily understandable format (S213). A model that can summarize this information would be valuable to analysts well beyond the IC. (S214).</p>"
S108,"<p>Question 2.6i Given a packet capture file print out, can a LLM summarize the network interactions? Scope: short to medium-term (S215).</p>"
S109,"<p>Question 2.6j Given the source code of a program, can an LLM explain what the program does? (S216).</p>"
S110,"<p>Question 2.9b Given the disassembler output of an unknown program, can a LLM explain what it does?  (S217).</p>"
S111,<p>2.7 Data  (S218).</p>
S112,<p>DUC 2001-2007 and TAC 2008-2011 datasets (S219).</p>
S113,"<p>CrisisFACTS – These data were created for a task at TREC 2022 (S220). The main task is to retrieve relevant passages from documents to support relief during a FEMA emergency (S221). Relevant passages, i.e., extract summaries, are available (S222).</p>"
S114,<p>SummaC – Code with benchmarks are in https://github.com/tingofurro/summac. (S223).</p>
S115,<p>XSUM (Short one-sentence abstractive summaries) available via https://huggingface.co/ datasets/xsum (S224).</p>
S116,"<p>xsum-hallucination-annotations – This dataset contains faithfulness and factuality annotations of highly abstractive summaries for the XSUM dataset [23] (S225). Abstractive summaries were generated using 5 systems: PTGEN, TCONVS2S, GPT-TUNED, TRANS2S and BERTS2S (S226). Raters were shown the news article and the system summary, and were tasked with identifying and annotating spans that weren’t supported by the input article (S227). Three judgements were crowd sourced for each of 500 x 5 document-system pairs., (S228).</p>"
S117,"<p>NewsQA – This is a question answering scenario with factual questions for news articles [24] (S229). The dataset contains 12,744 CNN news articles and 119,633 question-answer pairs (S230). Questions were written by humans who only saw headlines and article highlights (S231). Answers were written by humans who only saw the questions and the entire article, and selected a span of text or “No Answer” if the question was not answerable., (S232).</p>"
S118,"<p>NaturalQuestions – This is a question answering scenario with naturally occurring questions [25] (S233). Questions were real questions issued to the Google search engine (S234). A human annotator was presented with a question and a related Wikipedia page, and selected a long answer (typically a paragraph) and a short answer if present, or null if no long/short answer was available (S235). The public release of the dataset consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotations sequestered as test data (S236). This is best understood by looking at an example at https://ai.google.com/research/NaturalQuestions/visualization (S237). The question, “when are hops added to the brewing process?” is accompanied by the Brewing Wikipedia page (S238). The long answer is, “After mashing, the beer wort is boiled with hops...” The short answer is, “The boiling process”., (S239).</p>"
S119,"<p>NarrativeQA – This is a question answering scenario with long documents and summaries collected from books and movie scripts [26] (S240). Books were obtained from Project Gutenberg and scripts were scraped from the web (S241). Question answer pairs were generated by humans using human written plot summaries from Wikipedia (S242). The dataset has 1,567 stories (books or movie scripts), and 46765 question answer pairs.,  (S243).</p>"
S120,"<p>CNN/DailyMail – The CNN/DM dataset is an English-language corpus containing just over 300k unique news articles as written by journalists at CNN and the United Kingdom’s Daily Mail (Hermann et al., 2015; Huggingface.co, 2020) (S244). The CNN articles were written between 2007 and 2015 while the Daily Mail articles were written between 2010 and 2015.[27]  (S245).</p>"
S121,"<p>Cyber Threat Intelligence data: Consists of cyber threat intelligence information in spreadsheets and plaintext, with supporting documents and associated question-answer pairs (S246).</p>"
S122,"<p>Zendia Synthetic Dataset – Generated during SCADS 2023, this dataset contains roughly 5000 reports generated with GPT 3.5 turbo and GPT 4 (S247). It follows a fictional conflict, and has reports addressing a variety of events during this fictional situation (S248).</p>"
S123,"<p>MS MARCO v2 Passage Ranking – Created by Microsoft, contains 8.8 million passages with the objective to rank them based on relevance to a given question [28] (S249). Will be used to evaluate the TREC 2024 RAG track, supplemented with some additional ‘hard’ questions (S250). Standard MS MARCO dataset is available from Microsoft or Huggingface Datasets, supplemental TREC data will be available as well (S251).</p>"
S124,"<p>NeuCLIR1 - Created for the TREC 2022 NeuCLIR Track, this will also be used for the 2024 NeuCLIR track as well (S252). Contains web pages from Common Crawl in Chinese, Persian, and Russian (S253). Available through Huggingface Datasets (S254).</p>"
S126,<p>References  (S255).</p>
S127,"<p>[1] White, C.T.; Molino, N.P.; Yang, J.S.; Conroy, J.M (S256). occams: A Text Summarization Package (S257). Analytics 2023, 2, 546-559 (S258).</p>"
S128,"<p>[2] M (S259). Gambhir and V (S260). Gupta, “Recent automatic text summarization techniques: A survey,” Artif (S261). Intell (S262). Rev., vol (S263). 47, no (S264). 1, pp (S265). 1–66, Jan (S266). 2017, issn: 0269-2821 (S267). doi: 10.1007/s10462-016-9475-9 (S268). [Online] (S269). Available:  (S270).</p>"
S129,"<p>[3] W (S271). S (S272). El-Kassas, C (S273). R (S274). Salama, A (S275). A (S276). Rafea, and H (S277). K (S278). Mohamed, “Automatic text summarization: A comprehensive survey,” Expert Systems with Applications, vol (S279). 165, p (S280). 113 679, 2021, issn: 0957-4174 (S281). doi: https://doi.org/10.1016/j.eswa.2020.113679 (S282). [Online] (S283). Available:  (S284).</p>"
S130,"<p>[4] J (S285). Devlin, M.-W (S286). Chang, K (S287). Lee, and K (S288). Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota: Association for Computational Linguistics, Jun (S289). 2019, pp (S290). 4171–4186 (S291). doi: 10.18653/v1/N19-1423 (S292). [Online] (S293). Available:  (S294).</p>"
S131,"<p>[5] Lewis, P., et al (S295). (2020) (S296). Retrieval-augmented generation for knowledge-intensive NLP tasks (S297). Advances in Neural Information Processing Systems, 33, 9459–9474 (S298).</p>"
S132,"<p>[6] Monigatti, Leonie (S299). Retrieval-Augmented Generation Workflow (S300). 14 Nov (S301). 2024 (S302). Towards Data Science, https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2 (S303). Accessed 14 Mar (S304). 2024 (S305).</p>"
S133,"<p>[7] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston (S306). 2021 (S307). Retrieval augmentation reduces hallucination in conversation (S308). arXiv preprint arXiv:2104.07567  (S309).</p>"
S134,<p>[8] Anoushka Gade and Jorjeta Jetcheva (S310). 2024 (S311). It’s About Time: Incorporating Temporality in Retrieval Augmented Language Models (S312). ArXiv preprint arXiv:2401.13222  (S313).</p>
S135,"<p>[9] J (S314). Steinberger, M (S315). Poesio, M (S316). A (S317). Kabadjov, and K (S318). Ježek, “Two uses of anaphora resolution in summarization,” Information Processing and Management, vol (S319). 43, no (S320). 6, pp (S321). 1663–1680, 2007, Text Summarization, issn: 0306-4573 (S322). doi: https://doi.org/10.1016/j.ipm.200.01.010 (S323). [Online] (S324). Available:  (S325).</p>"
S136,"<p>[10] L (S326). Ouyang, J (S327). Wu, X (S328). Jiang, et al., “Training language models to follow instructions with human feedback,” arXiv e-prints, arXiv:2203.02155, arXiv:2203.02155, Mar (S329). 2022 (S330). doi: 10.48550/ arXiv.2203.02155 (S331). arXiv: 2203.02155 [cs.CL] (S332).</p>"
S137,"<p>[11] T (S333). Zhang, F (S334). Ladhak, E (S335). Durmus, P (S336). Liang, K (S337). McKeown, and T (S338). B (S339). Hashimoto, Benchmarking large language models for news summarization, 2023 (S340). doi: 10.48550/ARXIV.2301.13848 (S341). [Online] (S342). Available:  (S343).</p>"
S138,"<p>[12] P (S344). Liang, R (S345). Bommasani, T (S346). Lee, et al., Holistic evaluation of language models, 2022 (S347). doi: 10 (S348). 48550/ARXIV.2211.09110 (S349). [Online] (S350). Available:  (S351).</p>"
S139,"<p>[13] P (S352). Laban, T (S353). Schnabel, P (S354). N (S355). Bennett, and M (S356). A (S357). Hearst, “SummaC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization,” Transactions of the Association for Computational Linguistics, vol (S358). 10, pp (S359). 163–177, Feb (S360). 2022, issn: 2307-387X (S361). doi: 10.1162/tacl_a_00453 (S362). eprint: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\ _00453/1987014/tacl\_a\_00453.pdf (S363). [Online] (S364). Available:  (S365).</p>"
S140,"<p>[14] A (S366). R (S367). Fabbri, C (S368). Wu, W (S369). Liu, and C (S370). Xiong, “Qafacteval: Improved qa-based factual consistency evaluation for summarization,” CoRR, vol (S371). abs/2112.08542, 2021 (S372). arXiv: 2112.08542 (S373). [Online] (S374). Available: . (S375).</p>"
S141,"<p>[15]  Z (S376). Guo, M (S377). Schlichtkrull, and A (S378). Vlachos, “A Survey on Automated Fact-Checking,” Transactions of the Association for Computational Linguistics, vol (S379). 10, pp (S380). 178–206, Feb (S381). 2022, issn: 2307-387X (S382). doi: 10.1162/tacl_a_00454 (S383). eprint: https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00454/1987018/tacl\ _a\_00454.pdf (S384). [Online] (S385). Available:  (S386).</p>"
S142,"<p>[16] Z (S387). Ji, N (S388). Lee, R (S389). Frieske, et al., “Survey of hallucination in natural language generation,” ACM Comput (S390). Surv., Nov (S391). 2022, Just Accepted, issn: 0360-0300 (S392). doi: 10.1145/3571730 (S393). [Online] (S394). Available:  (S395).</p>"
S143,"<p>[17] Liu, Y., Fabbri, A., Zhao, Y., Liu, P., Joty, S., Wu, C.-S., Xiong, C., & Radev, D (S396). (2023) (S397). Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation (S398). In H (S399). Bouamor, J (S400). Pino, & K (S401). Bali (Eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (pp (S402). 16360–16368) (S403). Association for Computational Linguistics (S404). . (S405).</p>"
S144,"<p>[18] P (S406). Manakul, A (S407). Liusie, and M (S408). J (S409). F (S410). Gales, “Mqag: Multiple-choice question answering and generation for assessing information consistency in summarization,” ArXiv, vol (S411). abs/2301.12307, 2023 (S412).</p>"
S145,"<p>[19] H (S413). Sun, “Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond”, arXiv: 2310.06147 (S414). [Online] Available: . (S415).</p>"
S146,"<p>[20] H (S416). Chen, X (S417). Liu, D (S418). Yin, and J (S419). Tang, “A survey on dialogue systems: Recent advances and new frontiers,” SIGKDD Explor., vol (S420). 19, no (S421). 2, pp (S422). 25–35, 2017 (S423). doi: 10.1145/3166054.3166058 (S424). [Online] (S425). Available:  (S426).</p>"
S147,"<p>[21] D (S427). Tuggener, M (S428). Mieskes, J (S429). Deriu, and M (S430). Cieliebak, “Are we summarizing the right way? a survey of dialogue summarization data sets,” in Proceedings of the Third Workshop on New Frontiers in Summarization, Online and in Dominican Republic: Association for Computational Linguistics, Nov (S431). 2021, pp (S432). 107–118 (S433). doi: 10.18653/v1/2021.newsum-1.12 (S434). [Online] (S435). Available:  (S436).</p>"
S148,<p>[22] Wilson Yan and Yunzhi Zhang and Pieter Abbeel and Aravind Srinivas (S437). 2021 (S438). VideoGPT: Video Generation using VQ-VAE and Transformers (S439). ArXiv preprint arXiv:2104.10157  (S440).</p>
S149,"<p>[23] J (S441). Maynez, S (S442). Narayan, B (S443). Bohnet, and R (S444). McDonald, “On faithfulness and factuality in abstractive summarization,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online: Association for Computational Linguistics, Jul (S445). 2020, pp (S446). 1906–1919 (S447). doi: 10.18653/v1/2020.acl-main.173 (S448). [Online] (S449). Available:  (S450).</p>"
S150,"<p>[24] A (S451). Trischler, T (S452). Wang, X (S453). Yuan, et al., “NewsQA: A machine comprehension dataset,” in Proceedings of the 2nd Workshop on Representation Learning for NLP, Vancouver, Canada: Association for Computational Linguistics, Aug (S454). 2017, pp (S455). 191–200 (S456). doi: 10.18653/v1/W17-2623 (S457). [Online] (S458). Available:  (S459).</p>"
S151,"<p>[25] T (S460). Kwiatkowski, J (S461). Palomaki, O (S462). Redfield, et al., “Natural questions: A benchmark for question answering research,” Transactions of the Association for Computational Linguistics, vol (S463). 7, pp (S464). 452–466, 2019 (S465). doi: 10.1162/tacl_a_00276 (S466). [Online] (S467). Available:  (S468).</p>"
S152,"<p>[26] T (S469). Kočiský, J (S470). Schwarz, P (S471). Blunsom, et al., “The NarrativeQA reading comprehension challenge,” Transactions of the Association for Computational Linguistics, vol (S472). 6, pp (S473). 317–328, 2018 (S474). doi: 10.1162/tacl_a_00023 (S475). [Online] (S476). Available:  (S477).</p>"
S153,"<p>[27] Huggingface.co, CNN / Daily Mail - datasets at hugging face, 2020 (S478). [Online] (S479). Available: https: //. (S480).</p>"
S154,"<p>[28] Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., & others (S481). (2016) (S482). Ms marco: A human generated machine reading comprehension dataset (S483). ArXiv Preprint ArXiv:1611.09268 (S484).</p>"
S155,<p>3 Recommendation Systems  (S485).</p>
S156,"<p>To tailor content to the specific needs and interests of individual knowledge workers, the creation of TLDRs will involve a recommender system, which is a filtering technique that provides personalized item recommendations on a per-user basis (S486). The system can learn to predict “explicit” user feedback (e.g., thumbs up, star ratings) and/or “implicit” user feedback (e.g (S487). dwell time) (S488). Additional metadata about the user (e.g (S489). a knowledge worker’s organization designator or the information needs that they track) can be incorporated to further tailor the recommendations (S490). Metadata relating to the items is also useful for learning predictions (e.g (S491). document title, summary, author) (S492). In the context of recommender systems, user and item-related metadata is referred to as side information (S493). In some applications users’ interests can be dynamic, possibly over the course of a day, week, or month, making user context information appropriate for generating tailored daily reports (S494).</p>"
S157,"<p>Additionally, the recommendation system should be explainable as knowledge workers will want to know why particular items are being recommended (S495). Researchers at SCADS 2022 and 2023 developed methods to provide explainable document recommendations to knowledge workers (S496). We welcome researchers to build upon these results. (S497).</p>"
S158,<p>3.1 Recommender Systems Models (S498).</p>
S159,<p>Critical Challenge 3.A Consider methods for personalized content recommendation tasks related to the TLDR (S499). Scope: mid-to-long term  (S500).</p>
S160,<p>Question 3.1a Can large language models (LLMs) and/or knowledge graphs (KGs) be used to improve recommender systems prediction accuracy and/or diversity? Scope: short-to-mid-term (S501).</p>
S161,"<p>LLMs: A growing body of research focuses on the use of LLMs for the recommendation task, utilizing the powerful language understanding of these models (S502). Some of the research is promising, while others show no improvement over traditional methods (S503). See [20], [27], [28], and [29] for surveys of this topic (S504).</p>"
S162,"<p>Work in SCADS 2023 was a significant step in addressing the challenge of using LLMs (both zero-shot and fine-tuned) for recommendation, showing that a particular application of an encoder-decoder LLM suggested in [15] did not produce a strong news article recommender system (S505). However, more recent approaches to incorporating LLMs could be fruitful (S506).</p>"
S163,<p>KG: Knowledge graphs can be useful for injecting external knowledge into an article recommendation model for the purposes of learning information not found in article text or metadata (S507). In general there are three main categories of recommender systems that incorporate knowledge graphs [2]:  (S508).</p>
S164,<p>Embedding based: Pre-processed entity embeddings derived from knowledge graphs are used as input to the recommendation model (S509).</p>
S165,<p>Path-based: Item connection patterns in a knowledge graph are used to guide recommendations (S510).</p>
S166,<p>Hybrid: Combination of the previous two categories (S511).</p>
S167,"<p>Additionally, items for recommendation may be linked to a knowledge graph in a couple of ways:  (S512).</p>"
S168,<p>The items themselves may be present in a knowledge graph (S513). Movie recommendation is an example (S514). See [2] for additional examples (S515).</p>
S169,"<p>Item metadata (aka ”side information”) may be linked to the knowledge graph (S516). For example, in news article recommendation, named entity recognition (NER) can identify entities within the text, which are linked to entities in a knowledge graph (S517).</p>"
S170,<p>See [9] and [10] for surveys on knowledge graph-based recommender systems. (S518).</p>
S171,"<p>SCADS 2022 researchers utilized Deep Knowledge-Aware Network for News Recommendation (DKN) [3], incorporating embedded sub-graphs of a larger knowledge graph into the recommendation model (S519). However, this model did not show improvements in accuracy metrics over Neural News Recommendation Model with Multi-head Self Attention (NRMS) [1], a transformer-based model studied at SCADS 2022 and 2023, when applied to the MIcrosoft News Dataset (MIND) [16] (S520). Due to time constraints, beyond-accuracy metrics such as diversity were not calculated for DKN (S521).</p>"
S172,"<p>Additionally, researchers at SCADS 2023 investigated personalized news graphs [32] with a focus on explainability, but did not have time to compute the model accuracy or beyond-accuracy metrics; this could be extended in 2024 (S522). A challenge of this approach is the requirement to create separate news graphs for each user (S523). An alternate approach may be the  Global-LOcal news Recommendation sYstem (GLORY), presented in [21], which claims to beat NRMS [1] trained on MIND dataset in both accuracy (6.8% improvement) and diversity (1.2% improvement in NDCG@5). (S524).</p>"
S173,"<p>To address question 3.1a, SCADS 2024 researchers might consider the these additional research directions:  (S525).</p>"
S174,"<p>The OpeN- and ClosEd-source (ONCE) framework described in [22] utilizes LLMs to both augment the dataset (using closed source LLMs) and learn content representations (by fine-tuning open source LLMs) (S526). The content representation layers can be substituted for the representations previously proposed in models such as the NRMS [1] (S527). The authors claim a 24% improvement (in NDCG@5) over the original NRMS model trained on MIND, most of which was achieved by the representation learning with open source LLMs (S528).</p>"
S175,"<p>The LLM and KG for Personalized News Recommendation Framework (LKPNR), described in [17], showed a 6.1% improvement (in NDCG@5) compared to NRMS [1], when applied MIND (S529). The authors also motivate the model design by its ability to address the long tail problem (e.g (S530). 80% of user clicks are concentrated on 20% of items), which can lead to popularity biased recommendations (S531). They demonstrate this point through a case study, which could potentially be bolstered by beyond-accuracy metrics (S532). These results are achieved by incorporating both LLMs and KGs (based on text-extracted entities) into the model. (S533).</p>"
S176,"<p>Another related research topic is graph neural networks (GNNs) (S534). [33] states that, “GNN-based recommenders have achieved state-of-the-art in many aspects, including different recommendation stages, scenarios, objectives, and applications.” See [11] and [33] for surveys on GNNs. (S535).</p>"
S177,"<p>Additionally, researchers may consider ways to combine the best aspects of LLM-based models (e.g (S536). ONCE [22]), which achieve impressive accuracy improvements, and models that incorporate KGs (e.g (S537). LKPNR [18]), which are expected to achieve improved beyond-accuracy metrics, such as diversity, compared to an NRMS baseline (S538).</p>"
S179,<p>Question 3.1b How effective is reinforcement learning (RL) for personalized content recommendation in the TLDR application? (S539).</p>
S180,"<p>RL excels at responding to dynamic environments and capturing changes in user interests [30] (S540). Additionally, RL has shown promise in breaking the feedback loop (see question 3.2b), where competing goals such as exploitation of user interests (i.e (S541). optimization) and exploration can be explored [31] (S542). It would be beneficial to better understand how RL compares to other recommendation approaches studied at SCADS for the TLDR (S543). See [30] and [34] for surveys of RL and deep RL approaches for recommender systems (S544).</p>"
S182,"<p>Question 3.1c Can user side information (i.e (S545). metadata), context, and/or additional item side information  be incorporated into the recommender system to increase prediction accuracy? (S546).</p>"
S183,"<p>Of the models considered at SCADS 2022 and 2023, NRMS [1] was the best performing on the MIND dataset (according to accuracy metrics) but only incorporates article titles (S547). How might this model (or other models of interest) be expanded to incorporate additional item and user features, considering their effect on accuracy and beyond-accuracy metrics? Or can the learned user and item representation vectors from these models (e.g (S548). NRMS) be utilized as input layers to other models that incorporate additional features? A couple of notes on this topic: (S549).</p>"
S184,"<p>ONCE [22], proposed for investigation of question 3.1a, describes the use of pooling and a multilayer perceptron to incorporate additional features into NRMS and other similar models (see section 4.2 of the paper) (S550).</p>"
S185,"<p>LKPNR [17], also suggested as approach to question 3.1a, incorporates additional article information (e.g (S551). abstract, category, and sub-category) by concatenating with the title prior to embedding (S552). Alternatively, NAML [24] applies an attention mechanism to incorporate additional article features (S553).</p>"
S186,"<p>The table below lists examples of datasets containing side information that could be included in addition to textual features (such as title, body, summary, key words, and entities): (S554).</p>"
S188,"<p>Additionally, see the GENRE method described in [22] for generation of additional features using LLMs (S555).</p>"
S189,"<p>See also Question 3.1d, which is related. (S556).</p>"
S191,<p>Q3.1d How can multi-modal item information be used to improve a recommender system for the TLDR? (S557).</p>
S192,"<p>The availability of content beyond text, such as images, might be used to enhance the capability of a recommender system (S558). What is the best way to incorporate image content? And, if image content provides improvement, to what degree? The V-MIND dataset [35], a version of the MIND dataset with the addition of images, may be useful for this question (S559). See [36] for a survey on the topic of multi-modal recommendation (S560).</p>"
S194,<p>Question 3.1e How do various models compare according to offline and/or user tests? Scope: medium to long-term  (S561).</p>
S195,<p>One might think of evaluation of recommender systems in three phases: (S562).</p>
S196,<p>Offline evaluation: The model is trained on a static dataset producing accuracy metrics (computed on a hold-out test set) and beyond-accuracy metrics (computed on top-k recommendation lists). (S563).</p>
S197,"<p>User pilot/alpha/beta testing: Provide a set of new item recommendations to a set of users (S564). Goals could be as simple as feasibility testing with a small set of users (S565). If a sufficient number of users are included in the study, statistical significance in model comparison, including comparisons against baselines, could be achieved. (S566).</p>"
S198,<p>A/B testing: Usually performed as part of a production system for model comparisons (S567). A set of users are provided recommendations from model A while another set of users are provided recommendations from model B (S568). Statistical analysis of targeted measures (e.g (S569). revenue or implicit user feedback such as click-through-rate) is used to compare models. (S570).</p>
S199,"<p>Offline evaluation is usually the easiest, especially during the initial research phase (S571). However, accuracy (and beyond-accuracy) metrics are proxies for user satisfaction (S572). Therefore, as research progresses, it will be beneficial to perform user testing. (S573).</p>"
S200,<p>Some potential research objectives to consider for question 3.1e: (S574).</p>
S201,"<p>Model comparison: Use the approaches described above (offline evaluation, user studies, and A/B testing) to compare models (S575). The type of evaluation will depend on the phase of the research. (S576).</p>"
S202,<p>Offline evaluation:  (S577).</p>
S203,"<p>Accuracy metrics: What is the best sampling strategy for test set creation? It is common to sample implicit negative examples for test-set creation in order to avoid scoring the large set of negatives, which can be time consuming (S578). However, if sampling is not done appropriately, model comparisons may not be statistically sound (S579). See [6] for a discussion of sampling strategies for accuracy metrics (S580).</p>"
S204,"<p>Beyond-accuracy metrics: Many published papers simply report accuracy metrics, which may not tell the entire model performance story (S581). Thus, it can be informative to compute beyond-accuracy metrics for model comparisons (on the set of item recommendations) (S582). See [7] for a discussion of beyond-accuracy metrics (S583). Additionally, see [25] for an example use of content diversity in model evaluation (S584).</p>"
S205,<p>Comparison of offline vs user studies and/or A/B testing: How well do accuracy (and beyond-accuracy) metrics predict user satisfaction? A sufficient sample size of users is necessary to answer this question. (S585).</p>
S207,<p>3.2 User Feedback  (S586).</p>
S208,"<p>Feedback, either implicit user-item interactions or explicit ratings, is used to infer user preferences about items (S587). For example, if we observe a long dwell time by a user on an online article, we might conclude that they liked the article (an example of implicit feedback) (S588). Alternatively, if they provide a “thumbs up”, we obtain explicit information about their preference (S589). Recommender systems are trained to predict preference or ranking of items that users have not yet experienced. (S590).</p>"
S209,"<p>Critical Challenge 3.B User feedback: Develop and implement recommender models that account for various types of user feedback as well as address bias caused by feed-back loops (S591). Scope: long-term, Related: Section 4.4  (S592).</p>"
S210,<p>Question 3.2a What is the effect of differing types of implicit user feedback on recommendation accuracy? Scope: short term (S593).</p>
S211,"<p>During SCADS 2023, researchers considered systems that collect multiple types of user feedback as indication of user interest (S594). These feedback signals could have varying importance or even contradict one another (e.g (S595). a click may indicate some interest in the content, but a short dwell time may indicate otherwise) (S596). To address this, The Entire Space Multi-Task Model (ESMM) [25] was applied to the Tenrec dataset [13], which provides implicit and explicit user feedback in the form of clicks, likes, shares, follows, and favorites. (S597).</p>"
S212,"<p>On the other end of the spectrum, some systems simply log user clicks and no other types of feedback (S598). But, is this the most informative type of user feedback that could be collected? Could other forms of implicit feedback be used to train models that provide better recommendations? Along these lines, a criticism of the NRMS [1] model (studied at the two previous years of SCADS) trained on titles and user clicks (provided by the MIND dataset) is that it may lead to click-bait (S599). In theory, training this model on more informative user feedback has the potential to produce more satisfying recommendations (S600).</p>"
S213,"<p>Potential research direction: In addition to clicks, the Adressa dataset includes dwell time (and article length for normalization) (S601). Researchers could modify the final activation layer and loss functions of NRMS (or any other model under consideration at SCADS) to model dwell time, comparing the outcome to the model trained on clicks (S602). If improvement is demonstrated, it could be used as motivation for system owners to log user dwell time (S603). See related research in [26] and [37[. (S604).</p>"
S215,<p>Question 3.2b Can we develop and implement recommender models that address bias caused by feedback loops? Scope: long-term  (S605).</p>
S216,"<p>Feedback loops are inherent in recommender systems (S606). For example, a knowledge worker may click to view an article which was used to generate their TLDR (S607). In the process the knowledge worker may provide indication that they ”liked” this content and the article is subsequently incorporated into future training iterations to produce new TLDRs (S608). The process repeats itself (S609). Investigate methods for mitigating this bias (S610).</p>"
S217,"<p>Potential research directions: Researchers at SCADS 2022 and 2023 investigated natural language inference (NLI) for recommending articles that contradict those that are provided by a recommender system (S611). This may help knowledge workers avoid bias by providing competing information/viewpoints (S612). This could be continued (S613). Another approach may be reinforcement learning to balance optimization with exploration [31] (S614). Additional commentary and ideas can be found in [7], [12], and [14] (S615).</p>"
S218,<p>See also: Questions 3.1b and 5.1a. (S616).</p>
S221,<p>3.3 Recommender Systems Explainability  (S617).</p>
S222,"<p>Critical Challenge 3.C Develop and implement recommender models that are explainable, enabling better understanding of how user interests are utilized to generate the TLDR (S618). Scope: long-term  (S619).</p>"
S223,"<p>The benefits of explainability in recommender systems include transparency, persuasiveness, effectiveness, trustworthiness, and satisfaction of recommendations (S620). Additionally, it facilitates system debugging by system designers (S621). [8] (S622).</p>"
S224,"<p>There exist two main classes of model explainability: model-intrinsic and model-agnostic (aka post-hoc) (S623). Model-intrinsic methods utilize interpretable models, which involve transparent decision mechanisms (S624). In contrast, model-agnostic methods are used to explain black-box models, such as complex neural networks (S625).</p>"
S225,"<p>SCADS 2022 researchers applied a model-agnostic approach, Local Interpretable Model-agnostic Explanations (LIME) [4], to news recommendations produced by the NRMS [1] model trained on the MIND dataset (S626). Working with the same model and dataset, SCADS 2023 researchers investigated a model-instrisic approach by interpreting the attention mechanism of the transformer-based model; this can be extended to SCADS 2024 (S627).</p>"
S226,<p>Question 3.3a How can we utilize knowledge graphs (KG) for explainable recommendation? Scope: medium-term  (S628).</p>
S227,"<p>As discussed under Question 3.1a, SCADS 2022 researchers utilized Deep Knowledge-Aware Network for News Recommendation (DKN) [3], an example of the embedding-based use of KGs, but did not pursue explanations (S629). During SCADS 2023, researchers employed personalized news graphs [32], demonstrating explainability through a use case arguing that graph visualization aids in explainability (S630). However, due to time constraints, explanation was not studied  in depth or at a large scale (S631).</p>"
S228,"<p>For any of the KG-based recommender systems, used during current or former iterations of SCADS, demonstrate automated generation of recommendation explanations and evaluate their usefulness. (S632).</p>"
S230,<p>Question 3.3b What additional methods can be used to explain recommendations? (S633).</p>
S231,<p>See [8] for a survey of explainable recommender systems. (S634).</p>
S233,<p>Question 3.3c How do explanation approach(es) explored during previous and current iterations of SCADS compare and contrast to each other? Scope: medium-term  (S635).</p>
S234,<p>See [5] for a survey on the evaluation of explainable recommendation (S636).</p>
S236,"<p>Question 3.3d Given examples of neural models that 1) accurately predict user preference, 2) provide diverse recommendations, 3) are flexible in the type of user feedback that they model, 4) account for the feedback loop, and 5) are explainable, can we develop model(s) that perform all of these tasks? Scope: long-term  (S637).</p>"
S237,"<p>Published papers often focus on one objective, however we care about all these objectives (S638). Neural networks are modular, meaning that their sub components can be used as building blocks to build models that meet all of the objectives of the SCADS Grand Challenge (S639). Consider methods to combine techniques used to address challenges 3.A, 3.B, and 3.C. (S640).</p>"
S240,<p>3.4 Data (S641).</p>
S241,<p>MIND [16]: A large-scale dataset for English news recommendation (S642). Includes user impressions and views in addition to article content and metadata (S643).</p>
S242,<p>V-MIND: [35]: A multi-modal (images and text) version of the MIND dataset (S644).</p>
S243,"<p>Adressa [18]: A large-scale dataset for Norwegian news recommendation (S645). In addition to article content and metadata, user side/context information is included (S646). Also, contains dwell time in addition to clicks as implicit feedback (S647).</p>"
S244,"<p>Tenrec [13]: Composed of user interactions with multimodal items (videos and text articles) (S648). Also includes user information and multiple types of user feedback (S649). This dataset currently does not contain content (e.g (S650). article titles, text, or video content). (S651).</p>"
S245,"<p>CiteULike [19]: Academic paper citations saved by CiteULike users (S652). Includes article text, citations, and user-defined tags (S653).</p>"
S246,<p>Wikidata (): Structured data useful for constructing knowledge graphs. (S654).</p>
S248,<p>References  (S655).</p>
S249,"<p>[1] C (S656). Wu, F (S657). Wu, S (S658). Ge, T (S659). Qi, Y (S660). Huang, and X (S661). Xie, “Neural News Recommendation with Multi Head Self-attention,” in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China: Association for Computational Linguistics, Nov (S662). 2019, pp (S663). 6389–6394 (S664). doi: 10.18653/v1/D19-1671 (S665). [Online] (S666). Available: https://aclanthology.org/ D19-1671 (S667).</p>"
S250,"<p>[2] H (S668). Wang, F (S669). Zhang, M (S670). Zhang, et al., “Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems,” in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser (S671). KDD ’19, Anchorage, AK, USA: Association for Computing Machinery, 2019, pp (S672). 968–977, isbn: 9781450362016 (S673). doi: 10  (S674). 1145 / 3292500  (S675). 3330836 (S676). [Online] (S677). Available:  (S678).</p>"
S251,"<p>[3] H (S679). Wang, F (S680). Zhang, X (S681). Xie, and M (S682). Guo, “DKN: Deep Knowledge-aware Network for News Recommendation,” in Proceedings of the 2018 World Wide Web Conference, ser (S683). WWW ’18, Lyon, France: International World Wide Web Conferences Steering Committee, 2018, pp (S684). 1835–1844, isbn: 9781450356398 (S685). doi: 10.1145/3178876.3186175 (S686). [Online] (S687). Available:  (S688).</p>"
S252,"<p>[4] M (S689). T (S690). Ribeiro, S (S691). Singh, and C (S692). Guestrin, “’Why Should I Trust You?’: Explaining the predictions of any classifier,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser (S693). KDD ’16, San Francisco, California, USA: Association for Computing Machinery, 2016, pp (S694). 1135–1144, isbn: 9781450342322 (S695). doi: 10.1145/2939672.2939778 (S696). [Online] (S697). Available:  (S698).</p>"
S253,"<p>[5] X (S699). Chen, Y (S700). Zhang, and J (S701). Wen, “Measuring ‘Why’ in Recommender Systems: A Comprehensive Survey on the Evaluation of Explainable Recommendation,” ArXiv, vol (S702). abs/2202.06466, 2022 (S703). Available:  (S704).</p>"
S254,"<p>[6] W (S705). Krichene and S (S706). Rendle, “On Sampled Metrics for Item Recommendation,” ser (S707). KDD ’20, Virtual Event, CA, USA: Association for Computing Machinery, 2020, pp (S708). 1748–1757, isbn: 9781450379984 (S709). doi: 10.1145/3394486.3403226 (S710). [Online] (S711). Available:  (S712).</p>"
S255,"<p>[7] S (S713). Raza and C (S714). Ding, “News Recommender System: A Review of Recent Progress, Challenges, and Opportunities,” Artificial Intelligence Review, vol (S715). 55, 1 2022 (S716). [Online] (S717). Available: https: // (S718).</p>"
S256,"<p>[8] Y (S719). Zhang and X (S720). Chen, “Explainable Recommendation: A Survey and New Perspectives,” CoRR, vol (S721). abs/1804.11192, 2018 (S722). arXiv: 1804.11192 (S723). [Online] (S724). Available:  (S725).</p>"
S257,"<p>[9] Q (S726). Guo, F (S727). Zhuang, C (S728). Qin, et al., “A Survey on Knowledge Graph-based Recommender Systems,” IEEE Transactions on Knowledge and Data Engineering, vol (S729). 34, no (S730). 08, pp (S731). 3549–3568, Aug (S732). 2022, issn: 1558-2191 (S733). doi: 10.1109/TKDE.2020.3028705 (S734). Available:  (S735).</p>"
S258,"<p>[10] D (S736). Li, H (S737). Qu and J (S738). Wang, ""A Survey on Knowledge Graph-Based Recommender Systems,"" 2023 China Automation Congress (CAC), doi: 10.1109/CAC59555.2023.10450693 (S739). Available:  (S740).</p>"
S259,"<p>[11] S (S741). Wu, F (S742). Sun, W (S743). Zhang, X (S744). Xie, and B (S745). Cui, “Graph Neural Networks in Recommender Systems: A Survey,” ACM Comput (S746). Surv., vol (S747). 55, no (S748). 5, Dec (S749). 2022, issn: 0360-0300 (S750). doi: 10.1145/3535101 (S751). [Online] (S752). Available:  (S753).</p>"
S260,"<p>[12] Z (S754). Zhao, L (S755). Hong, L (S756). Wei, et al., “Recommending What Video to Watch Next: A Multitask Ranking System,” in Proceedings of the 13th ACM Conference on Recommender Systems, ser (S757). Rec Sys ’19, Copenhagen, Denmark: Association for Computing Machinery, 2019, pp (S758). 43–51, isbn: 9781450362436 (S759). doi: 10.1145/3298689.3346997 (S760). [Online] (S761). Available:  (S762).</p>"
S261,"<p>[13] G (S763). Yuan, F (S764). Yuan, Y (S765). Li, et al., “Tenrec: A Large-scale Multipurpose Benchmark Dataset for Recommender Systems,” in Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022 (S766). [Online] (S767). Available:  (S768).</p>"
S262,"<p>[14] J (S769). Stray, “The AI Learns to Lie to Please You: Preventing Biased Feedback Loops in Machine-assisted Intelligence Analysis,” in 2022 Summer Conference on Applied Data Science, 2022 (S770).</p>"
S263,"<p>[15] S (S771). Geng, S (S772). Liu, Z (S773). Fu, Y (S774). Ge, and Y (S775). Zhang, “Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5),” in Proceedings of the 16th ACM Conference on Recommender Systems, ser (S776). RecSys ’22, Seattle, WA, USA: Association for Computing Machinery, 2022, pp (S777). 299–315, isbn: 9781450392785 (S778). doi: 10.1145/3523227 (S779). 3546767 (S780). [Online] (S781). Available:  (S782).</p>"
S264,"<p>[16] F (S783). Wu, Y (S784). Qiao, J.-H (S785). Chen, et al., “MIND: A Large-scale Dataset for News Recommendation,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online: Association for Computational Linguistics, Jul (S786). 2020, pp (S787). 3597–3606 (S788). doi: 10.18653/v1/2020 (S789). acl-main.331 (S790). [Online] (S791). Available: . (S792).</p>"
S265,"<p>[17] C (S793). Hao, X (S794). Runfeng, C (S795). Xiangyang, Y (S796). Zhou, W (S797). Xin, Xuanzhanwei, Z (S798). Kai (S799). “LKPNR: LLM and KG for Personalized News Recommendation Framework,” 2023 (S800). arXiv preprint arXiv:2308.12028 (S801). Available:  (S802).</p>"
S266,"<p>[18] Gulla, J (S803). A., Zhang, L., Liu, P., Özgöbek, Ö., & Su, X (S804). “The Adressa Dataset for News Recommendation,” 2017 (S805). In Proceedings of the International Conference on Web Intelligence (pp (S806). 1042-1048) (S807). ACM (S808). Available:  (S809).</p>"
S267,"<p>[19] Hao Wang and Binyi Chen and Wu-Jun Li (S810). “Collaborative Topic Regression with Social Regularization  for Tag Recommendation,” 2013 (S811). In Proceedings of DBLP:conf/ijcai/WangCL13 (S812). IJCAI (S813). Available:  (S814).</p>"
S268,"<p>[20] Zhaoxuan Tan and Meng Jiang (S815). “User Modeling in the Era of Large Language Models: Current Research and Future Directions,” 2023 (S816).  arXiv preprint arXiv:2312.11518 (S817). Available:  (S818).</p>"
S269,"<p>[21] B (S819). Yang, D Liu, T (S820). Suzumura, R Dong, and I (S821). Li (S822). “Going Beyond Local: Global Graph Enhanced Personalized News Recommendation,” 2023 (S823). In Proceedings of the 17th ACM Conference on Recommender Systems (pp (S824). 24-34) (S825). Available:  (S826).</p>"
S270,"<p>[22] Q (S827). Liu, N (S828). Chen, T (S829). Sakai, and X (S830). Wu (S831). “ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models,“ 2024 (S832). Proceedings of the 17th ACM International Conference on Web Search and Data Mining (S833). Available:  [31 (S834).</p>"
S271,"<p>[23] C (S835). Rudin (S836). “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead,” 2019 (S837). Nature Machine Intelligence, Vol 1 (S838). Available:  (S839).</p>"
S272,"<p>[24] C (S840). Wu, F Wu, M An, J (S841). Huang, Y Huang, and X (S842). Xie (S843). “Neural News Recommendation with Attentive Multi-view Learning”, 2019 (S844). In Proceedings of the 28th International Joint Conference on Artificial Inelligence (IJCAI’19) (S845). AAAI Press (S846). Available: . (S847).</p>"
S273,"<p>[25] X (S848). Ma, Z (S849). L (S850). Zhao, G (S851). Huang, Z (S852). Wang, Z (S853). Hu, X (S854). Zhu, and K (S855). Gai (S856). ""Entire space multi-task model: An effective approach for estimating post-click conversion rate"", 2018 (S857). In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp (S858). 1137-1140 (S859). 2018 (S860). Available:  (S861).</p>"
S274,"<p>[26] R (S862). Xie, L (S863). Ma, S (S864). Zhang, F (S865). Xia, and L (S866). Lin (S867). 2023 (S868). “Reweighting Clicks with Dwell Time in Recommendation”, 2023 (S869). In Companion Proceedings of the ACM Web Conference 2023 (WWW '23 Companion) (S870). Association for Computing Machinery, New York, NY, USA, 341–345 (S871). Available:  (S872).</p>"
S275,"<p>[27] L (S873). Wu, Z (S874). Zheng, Z (S875). Qiu, H (S876). Wang, H (S877). Gu, T (S878). Shen, C (S879). Qin et al (S880). ""A Survey on Large Language Models for Recommendation,"" 2023 (S881). arXiv preprint arXiv:2305.19860) Available: . (S882).</p>"
S276,"<p>[28] W Fan, Z (S883). Zhao, J (S884). Li, Y (S885). Liu, X (S886). Mei, Y (S887). Wang, J (S888). Tang, and Q (S889). Li (S890). ""Recommender Systems in the Era of Large Language Models (LLMs),"" 2023 (S891). arXiv preprint arXiv:2307.02046 (S892). Available:  (S893).</p>"
S277,"<p>[29] J (S894). Lin, X (S895). Dai, Y (S896). Xi, W (S897). Liu, B (S898). Chen, X (S899).  Li, C (S900). Zhu et al (S901). ""How Can Recommender Systems Benefit from Large Language Models: A Survey,"" 2023 (S902). arXiv preprint arXiv:2306.05817 (S903). Available:  (S904).</p>"
S278,"<p>[30] Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, Xianzhi Wang (S905). “Deep Reinforcement Learning in Recommender Systems: A Survey and New Perspectives”, 2023 (S906). Knowledge-Based Systems, Volume 264, 110335, ISSN 0950-7051 (S907). Available:  (S908).</p>"
S279,"<p>[31] J (S909). Chen, H (S910). Dong, X (S911). Wang, F (S912). Feng, M (S913). Wang, and X (S914). He (S915). “Bias and Debias in Recommender System: A Survey and Future Directions,” 2023 (S916). ACM Trans (S917). Inf (S918). Syst (S919). Available:  (S920).</p>"
S280,"<p>[32] D (S921). Liu, T (S922). Bai, J (S923). Lian, X (S924). Zhao, G (S925). Sun, J (S926). Wen, and X (S927). Xie (S928). ""News Graph: An Enhanced Knowledge Graph for News Recommendation,"" 2019 (S929). In KaRS@CIKM, Available:  (S930).</p>"
S281,"<p>[33] C (S931). Gao, Y (S932). Zheng, N (S933). Li, Y (S934). Li, Y (S935). Qin, J (S936). Piao, Y (S937). Quan et al (S938). ""A Survey of Graph Neural Networks for Recommender Systems: Challenges, Methods, and Directions,"" 2023 (S939). ACM Transactions on Recommender Systems 1 (S940). Available:  (S941).</p>"
S282,"<p>[34] Y (S942). Lin et al., ""A Survey on Reinforcement Learning for Recommender Systems,"" 2023 (S943). In IEEE Transactions on Neural Networks and Learning Systems, Available:  (S944).</p>"
S283,"<p>[35] S (S945). Han, W (S946). Huang, and X (S947). Luan (S948). ""VLSNR: Vision-Linguistics Coordination Time Sequence-aware News Recommendation,"" 2022 (S949). arXiv preprint arXiv:2210.02946 (S950). Available:  (S951).</p>"
S284,"<p>[36] H (S952). Zhou, X (S953). Zhou, Z (S954). Zeng, L (S955). Zhang, and Z (S956). Shen (S957). ""A comprehensive survey on multimodal recommender systems: Taxonomy, evaluation, and future directions,"" 2023 (S958). arXiv preprint arXiv:2302.04473 (S959). Available:  (S960).</p>"
S285,"<p>[37] C (S961). Wu, F (S962). Wu, T (S963). Qi, and Y (S964). Huang, “User Modeling with Click Preference and Reading Satisfaction for News Recommendation,” 2020 (S965). in Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, doi: 10.24963/ijcai.2020/418 (S966). [Online] (S967). Available:  (S968). . (S969).</p>"
S296,<p>4 Human-Computer Interaction  (S970).</p>
S297,"<p>At the core of the TLDR is the notion that the human knowledge worker trusts the Tailored Daily Report to deliver the information most relevant to the worker (S971). Similarly, it is critical that the algorithms behind the creation of the TLDR anticipate the current and future needs of the individual knowledge worker and satisfy those needs (S972). A TDLR must also inculcate the appropriate level of trust between the models and the users, so that a human user is able to appropriately calibrate their decision making processes based on AI explanations and clear representations of model accuracy. (S973). Thus we welcome researchers in Human-Machine Interaction with a background in the creation of ML solutions which achieve buy-in from the human knowledge worker (S974).</p>"
S298,<p>4.1 Modeling the Analytic Ecosystem  (S975).</p>
S299,<p>From J (S976). Block et al.’s “Preliminary Perspectives on Information Passing in the Intelligence Community”  (S977).</p>
S300,"<p>Research in HCI has repeatedly demonstrated that, despite impressive computational capabilities, the practical value of new tools and technologies depends on the ability to integrate them into appropriate environments [1] (S978). This is non-trivial (S979). Intelligence analysts conduct analysis within a complex sociotechnical system [2], [3] (S980). Understanding the information ecosystem intelligence analysts work within is useful for gaining an awareness of what kinds of information they work with and from what apertures, with whom they work, information sources, how information is transformed, and where it is distributed [1], [4] (S981). Analysts work with numerous tools, processes, and types of information (S982). Analysis also requires human judgment and creativity on how to best utilize the available tools [5] (S983). Throughout the analytic process, analysts also frequently communicate with others—either synchronously or asynchronously [2], [6], [7] (S984). Requests for information can come in different forms and be initiated by different types of customers [2] (S985). Aspects of the workflow can also be highly collaborative, where communications with colleagues or subject matter experts are essential for filling in knowledge gaps [2], [8] (S986). The high variability and complexity of analysis operations create a major challenge for researchers and software engineers to optimize the benefits of their contributions (S987).</p>"
S301,"<p>Personalization is an essential component of the TLDR (S988). It is difficult to build a personalized system without knowing what needs to be personalized (S989). Specifically, we need to be able to extract dimensions or factors of personalization for a TLDR system that align with the analysts’ actual needs (S990). During SCADS 2022 and 2023, we interviewed multiple analysts to better understand information flow throughout the analytic process (S991). Preliminary analysis of this interview data revealed that there are both formal and informal types of information that analysts reference in their work, and that some influencing factors on the process include the size of the system and challenges disseminating information (S992). Other preliminary findings indicate that analysts’ work requests come in different formats (e.g., email, physical walk-ins), each with its own set of corresponding triggers (S993). One factor to consider for personalization in the TLDR, then, may be to infer the receipt of work requests based on these various triggers (S994). This in turn informs the types of automation and processing that need to happen for the TLDR to be most effective for a given user (S995).</p>"
S302,"<p>Question 4.1a Can we capture the various elements of the analytic workflow, identifying triggers and corresponding actions, that will aid in characterizing ways in which a TLDR could be personalized for a user? Scope: medium-term  (S996).</p>"
S303,"<p>Question 4.1b What is the relationship between workflow factors and presentation of a TLDR? For example, should the TLDR presentation vary based on the current state in an analytic workflow, an analyst’s cognitive load, or other identifiable factors? Scope: medium-term, Related: Section 4.4 (S997).</p>"
S304,"<p>Throughout the analytic workflows, there are points where analysts may take on increasingly loads of data into their working memories on a single task or between multiple tasks before points of documentation or task switching (S998). TDLR presentations may fluctuate depending on variables such as the load on an analyst’s working memory, degree of multi-tasking, or cognitive load. (S999).</p>"
S306,<p>4.1.1 Available Data and Prior Work  (S1000).</p>
S307,<p>2022 SCADS Interview Dataset  (S1001).</p>
S308,<p>2022 SCADS Special Issue Paper: J (S1002). Block et al (S1003). ”Preliminary Perspectives on Information Passing in the Intelligence Community”  (S1004).</p>
S309,<p>2022 Crouser/Ottley Decision-Maker Survey Dataset  (S1005).</p>
S310,<p>2023 White Paper: M (S1006). Bancilhon et al.’s ”Communicating Intel to Decision-Makers: Toward the Integration of Text and Charts in Reports”  (S1007).</p>
S311,<p>2024 “The Analyst Experience”  (S1008).</p>
S312,<p>4.2 Communicating Model Accuracy and Uncertainty  (S1009).</p>
S313,"<p>Critical Challenge 4.A Develop novel encoding techniques for displaying a model’s accuracy or confidence information as context for reported results (S1010). Scope: medium-term, Related: Sections 3.1  (S1011).</p>"
S314,"<p>From Crouser / Ottley’s 2023 LAS White Paper Although machine learning models are increasingly used to support decision-making, their outcomes are inherently probabilistic, meaning that a particular decision’s output is not guaranteed to be accurate (S1012). Further, the accuracy of an algorithm’s output depends partly on the historical data used to train it, notoriously marred by bias and input errors (S1013). Thus, a key measure of success for human-machine teaming is the human’s ability to calibrate when to trust or distrust the algorithm’s output — accepting a biased or inaccurate suggestion results in poor decisions that may be catastrophic in high-stakes decision-making scenarios (S1014).</p>"
S315,"<p>One method to assist analysts in calibrating their trust is by assigning a confidence score to the output, thereby communicating the probabilistic nature of an algorithm (S1015). In this challenge, we focus specifically on the use and interpretation of confidence scores and other mechanisms for communicating model accuracy and uncertainty (as opposed to developing novel metrics for evaluating model validity) (S1016). In particular, we are interested in exploring novel mechanisms for communicating model accuracy, uncertainty, and bias that are effective at calibrating analyst confidence in the model’s output:  (S1017).</p>"
S316,"<p>Question 4.2a What methods for displaying confidence scores are most helpful in calibrating a user’s trust in a TLDR system? Scope: medium-term, Related: Section 4.1 (S1018).</p>"
S317,"<p>Question 4.2b How does visual design impact a user’s decision to trust a TLDR system? Scope: medium-term, Related: Section 4.1 (S1019).</p>"
S318,"<p>Question 4.2c Does the visual encoding affect a user’s decision to either defer to an algorithm’s recommendation or to query the original source data? Scope: medium-term, Related: Section 4.1 (S1020).</p>"
S319,"<p>Additionally, we are interested in the interplay between getting information to the right person and delivering that information in the right way (S1021). Prior work suggests that these two objectives are not independent; rather, the optimal mode of delivery depends on both the analytic goals and the individual traits of the intended recipient (S1022). To that end, the following questions are also of interest:  (S1023).</p>"
S320,"<p>Question 4.2d Which cognitive traits are reliable predictors of trust behavior? Scope: medium-term, Related: Section 4.1 (S1024).</p>"
S321,"<p>Question 4.2e What is the relationship between analysts’ individual differences and the efficacy of various visual encodings of model confidence? Scope: medium-term, Related: Section 4.1 (S1025).</p>"
S322,"<p>Question 4.2f  How does trust in AI systems evolve over time, and with different levels of system performance? (S1026).</p>"
S323,<p>Question 4.2g What are the best metrics and models to measure and quantify trust in an AI system over time? Are there identifiable soft metrics that can be used to assess user trust? (S1027).</p>
S324,<p>4.2.1 Available Data and Prior Work  (S1028).</p>
S325,<p>2023 PANDAS Audio Transcription Prototype: Crouser/Ottley/Ha  (S1029).</p>
S326,<p>4.3 Model Explainability  (S1030).</p>
S327,<p>Critical Challenge 4.B Can we expand on existing capabilities for promoting trust through the use of explainable modeling techniques (in contrast to traditional “black box” approaches)? Scope: short-term  (S1031).</p>
S328,<p>Adapted from G (S1032). Forbes / R (S1033). J (S1034). Crouser’s “Metric Ensembles Aid in Explainability: A Case Study with Wikipedia Data”  (S1035).</p>
S329,"<p>As machine learning models become larger and more complex, it has become both more difficult and more important to be able to explain and interpret the results of those models, both to prevent model errors and to inspire confidence for end users of the model (S1036).</p>"
S330,<p>From Wikipedia page on Explainable AI:  (S1037).</p>
S331,"<p>https://en.wikipedia.org/wiki/Explainable_artificial_intelligence The algorithms used in machine learning (ML) can be differentiated into white-box and black-box algorithms [9] (S1038). White-box models are ML models that provide results that are understandable for experts in the domain (S1039). Black-box models, on the other hand, are opaque and their precise mechanisms may not be understood even by domain experts [10] (S1040). White-box algorithms are designed to follow three principles:  (S1041).</p>"
S332,<p>Transparency: “the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer” [11] (S1042).</p>
S333,<p>Interpretability: the underlying basis for decision-making can be communicated in a way that is understandable to humans [12]–[14] (S1043).</p>
S334,"<p>Explainability: explicit access to “the collection of features of the interpretable domain that have contributed for a given example to produce a decision (e.g., classification or regression)” [15] (S1044).</p>"
S335,"<p>If algorithms meet these requirements, they provide a basis for justifying decisions, tracking and thereby verifying them, improving the algorithms, and exploring new facts [16] (S1045). Unfortunately, recommender models that are fit to a single, complex metric can be a black box (S1046). For example, assume we have a model trained on the MIND dataset, such as NMRS, which was designed to maximize click rate based on the article title (S1047). If we then deploy this model to recommend new articles based on a user’s prior clicks, the resulting recommendations do not come with any inherent explanation for why they were recommended (S1048). This characteristic can be particularly problematic when the model provides recommendations that miss the mark, especially in applications where the consequences for missed information are high; because the user is not provided with an explanation, “oddball” recommendations can quickly contribute to the erosion of trust in the system [17] (S1049).</p>"
S336,"<p>However, consider an alternative where instead of using a single black-box metric to measure fitness, we employ a suite of several complementary metrics or datasets and train different models to optimize for each of them (S1050). In cases where the individual models diverge in their evaluation of a candidate article, then the fact that these metrics were divergent tells us something about the article itself, and the particular ways in which it is potentially useful: the ensemble model can simply report, for its top candidates, which particular metrics that candidate satisfies, and to what degree (S1051). This gives the analyst a better idea about the “reasoning” behind the recommendations, which in turn can help to both generate confidence in the model when it is performing well and preserve trust in cases where it has failed (S1052). Moreover, a deeper understanding of why a given recommendation was made affords opportunities for additional model tuning and refinement (S1053). In conjunction with the Critical Challenge outlined in Section 4.2, we invite exploration into how the use of explainable models and/or ensemble metrics can be used to help calibrate trust in the output of computational models (S1054).</p>"
S337,"<p>Question 4.3a What methods for displaying explanations of model output are most helpful in calibrating a user’s trust in a TLDR system? Scope: medium-term, Related: Section 3.1  (S1055).</p>"
S338,"<p>Question 4.3b What other metrics beyond trust scores are helpful in explaining models, and how can that information be efficiently relayed to analysts? (S1056).</p>"
S339,<p>4.3.1 Available Data and Prior Work  (S1057).</p>
S340,<p>2022 SCADS Special Issue Paper: G (S1058). Forbes and R.J (S1059). Crouser’s “Metric Ensembles Aid in Explainability: A Case Study with Wikipedia Data”  (S1060).</p>
S341,<p>4.4 Interaction Design  (S1061).</p>
S342,"<p>It will be important to create a TLDR that people like and want to use (S1062). In addition to providing relevant high-quality content, the manner in which users interact with a TLDR will also be an influential factor in user acceptance (S1063).</p>"
S343,"<p>Critical Challenge 4.C Develop an interface such that a user can provide feedback on TLDR content (S1064). Scope: medium-term, Related: Section 3.2  (S1065).</p>"
S344,<p>Question 4.4a How can we design more intuitive and natural interfaces for multi-modal systems? (S1066).</p>
S345,<p>Question 4.4b How can graphic information be combined with natural language text to produce more effective and nuanced summaries for specialized knowledge workers? Can the use of natural language text summarization with graphic content affect cognitive load for knowledge workers? (S1067).</p>
S346,<p>Question 4.4c How can AI and humans effectively collaborate in decision-making processes? How can we identify and assess areas of effective AI-human collaboration? What are principles upon which we can build collaboration? (S1068).</p>
S347,<p>Question 4.4d How can we best collect and leverage analyst input and interaction data to improve outcomes? (S1069).</p>
S348,<p>Critical Challenge 4.D Develop an interface such that a user can navigate from summaries to the underlying documents (S1070). Scope: medium-term  (S1071).</p>
S349,"<p>Question 4.4e What type of presentation is most effective for providing users a summary of a conversational audio file? Scope: medium-term, Related: Sections 2.6, 4.2, 4.3 (S1072).</p>"
S350,<p>References  (S1073).</p>
S351,"<p>[1] S (S1074). Hepenstal, L (S1075). Zhang, and B (S1076). L (S1077). William Wong, “An analysis of expertise in intelligence analysis to support the design of human-centered artificial intelligence,” in 2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Oct (S1078). 2021, pp (S1079). 107–112 (S1080). doi: 10.1109/ SMC52423.2021.9659095 (S1081).</p>"
S352,"<p>[2] R (S1082). Johnston, Analytic culture in the US intelligence community: An ethnographic study (S1083). Central Intelligence Agency, 2005 (S1084).</p>"
S353,"<p>[3] B (S1085). R (S1086). Nolan, “Information sharing and collaboration in the United States intelligence community: An ethnographic study of the national counterterrorism center,” Ph.D (S1087). dissertation, University of Pennsylvania Unpublished PhD Dissertation, 2013 (S1088).</p>"
S354,"<p>[4] C (S1089). Groenewald, B (S1090). L (S1091). W (S1092). Wong, S (S1093). Attfield, P (S1094). Passmore, and N (S1095). Kodagoda, “How analysts think: How do criminal intelligence analysts recognise and manage significant information?” In 2017 European Intelligence and Security Informatics Conference (EISIC), Sep (S1096). 2017, pp (S1097). 47–53 (S1098). doi: 10.1109/EISIC.2017.15 (S1099).</p>"
S355,"<p>[5] G (S1100). Chin Jr, O (S1101). A (S1102). Kuchar, and K (S1103). E (S1104). Wolf, “Exploring the analytical processes of intelligence analysts,” in Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, Boston MA USA: ACM, Apr (S1105). 2009 (S1106).</p>"
S356,"<p>[6] S (S1107). Straus, A (S1108). Parker, and J (S1109). Bruce, “The group matters: A review of processes and outcomes in intelligence analysis,” Group Dynamics: Theory, Research, and Practice, vol (S1110). 15, pp (S1111). 128–146, Jun (S1112). 2011 (S1113). doi: 10.1037/a0022734 (S1114).</p>"
S357,"<p>[7] P (S1115). E (S1116). Keel, “Collaborative visual analytics: Inferring from the spatial organization and collaborative use of information,” in 2006 IEEE Symposium On Visual Analytics Science And Technology, Oct (S1117). 2006, pp (S1118). 137–144 (S1119). doi: 10.1109/VAST.2006.261415 (S1120).</p>"
S358,"<p>[8] K (S1121). M (S1122). Vogel and B (S1123). B (S1124). Tyler, “Interdisciplinary, cross-sector collaboration in the US Intelligence Community: Lessons learned from past and present efforts,” en, Intelligence and National Security, vol (S1125). 34, no (S1126). 6, pp (S1127). 851–880, Sep (S1128). 2019, issn: 0268-4527, 1743-9019 (S1129). doi: 10.1080/02684527.2019.1620545 (S1130). [Online] (S1131). Available:  (S1132).</p>"
S359,"<p>[9] G (S1133). Vilone and L (S1134). Longo, “Classification of explainable artificial intelligence methods through their output formats,” Machine Learning and Knowledge Extraction, vol (S1135). 3, no (S1136). 3, pp (S1137). 615–661, 2021 (S1138).</p>"
S360,"<p>[10] O (S1139). Loyola-Gonzalez, “Black-box vs (S1140). white-box: Understanding their advantages and weaknesses from a practical point of view,” IEEE access, vol (S1141). 7, pp (S1142). 154 096–154 113, 2019 (S1143).</p>"
S361,"<p>[11] R (S1144). Roscher, B (S1145). Bohn, M (S1146). F (S1147). Duarte, and J (S1148). Garcke, “Explainable machine learning for scientific insights and discoveries,” Ieee Access, vol (S1149). 8, pp (S1150). 42 200–42 216, 2020 (S1151).</p>"
S362,"<p>[12] W (S1152). J (S1153). Murdoch, C (S1154). Singh, K (S1155). Kumbier, R (S1156). Abbasi-Asl, and B (S1157). Yu, “Interpretable machine learning: Definitions, methods, and applications,” arXiv preprint arXiv:1901.04592, 2019 (S1158).</p>"
S363,"<p>[13] Z (S1159). C (S1160). Lipton, “The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery.,” Queue, vol (S1161). 16, no (S1162). 3, pp (S1163). 31–57, 2018 (S1164).</p>"
S364,"<p>[14] A (S1165). B (S1166). Arrieta, N (S1167). Díaz-Rodríguez, J (S1168). Del Ser, et al., “Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible ai,” Information fusion, vol (S1169). 58, pp (S1170). 82–115, 2020 (S1171).</p>"
S365,"<p>[15] G (S1172). Montavon, W (S1173). Samek, and K.-R (S1174). Müller, “Methods for interpreting and understanding deep neural networks,” Digital signal processing, vol (S1175). 73, pp (S1176). 1–15, 2018 (S1177).</p>"
S366,"<p>[16] A (S1178). Adadi and M (S1179). Berrada, “Peeking inside the black-box: A survey on explainable artificial intelligence (xai),” IEEE access, vol (S1180). 6, pp (S1181). 52 138–52 160, 2018 (S1182).</p>"
S367,"<p>[17] Y (S1183). Zhang, X (S1184). Chen, et al., “Explainable recommendation: A survey and new perspectives,” Foundations and Trends® in Information Retrieval, vol (S1185). 14, no (S1186). 1, pp (S1187). 1–101, 2020. (S1188).</p>"
S369,<p>5 Data Set Creation and/or Augmentation  (S1189).</p>
S370,"<p>Research into each of the preceding areas of TLDR creation requires data (S1190). A particular challenge is finding data sets appropriate for use in all of the SCADS research areas at a scale that enables meaningful outcomes (S1191). Creating a data set specific to the SCADS grand challenge would enable researchers to utilize the same data and represent an end-to-end TLDR use case (S1192). Therefore, we also welcome researchers who could build on prior SCADS outcomes to create new data sets, or augment existing data sets, that support TLDR creation (S1193). We list below a non-comprehensive set of research questions relevant to TLDR focus areas that would benefit from new or updated data sets (S1194). While we assume most data set creation and/or augmentation would focus on English text data, we welcome efforts that incorporate multi-modal and multi-lingual data (S1195).</p>"
S371,<p>5.1 Data Set Creation (S1196).</p>
S372,"<p>During the day, an analyst may interact with finished reporting (like news articles), but may also see individual pieces of content, such as communications (including metadata) or location data, and this data may be in audio, video, or text format (S1197). Any given data set need not be (and probably should not be) comprehensive, but it is important that overall we have data that mirrors the knowledge worker’s experience (S1198). While individual analysts will have different requirements (a signals analyst and cybersecurity analyst will not view the same data, for example), our data collection should be sufficiently flexible to accommodate multiple roles. (S1199).</p>"
S373,<p>Critical Challenge 5.A: Can we curate a collection of data sets that are analogous to what an analyst might have available as part of their work? Scope: short-term (S1200).</p>
S374,"<p>Data sets might also reflect preferences of the users themselves (S1201). A relevant data set might include information on a user’s motivation for interacting with an item in a data set; for example, was the item recommended to the user or did the user discover that item through some other mechanism? If an item was recommended, how did that item rank in comparison to other recommended items?  (S1202).</p>"
S375,<p>Question 5.1a Can we create a data set that enables research on preventing feedback loops in recommender systems? Scope: medium-term  (S1203).</p>
S376,"<p>Data sets that provide information on user preferences between different summaries would allow us to better understand the types of details that different users want to have included in tailored summaries (S1204). If the data set also provided some information about the individual users, we might be able to leverage that information to better understand the interaction between recommendation and summarization processes, how to optimize TLDRs for each user, and the extent to which tailored summaries are interchangeable between different users (S1205).</p>"
S377,"<p>Question 5.1b Can we create a data set that enables research on personalizing summaries of recommended content to accommodate an individual’s preferences? Scope: medium-term, Related: Section 2  (S1206).</p>"
S378,"<p>Large language models (LLMs) and other generative models are promising components of a TLDR system, though are prone to creating output that is not accurate or factual (S1207). To be able to use such models in creating TLDRs it will be necessary to be able to automatically identify false and inconsistent information in LLM output (S1208). This will require a data set that flags hallucinated information, and ideally categorizes the hallucinations according to a defined set, such as that in [1] (S1209).</p>"
S379,"<p>Question 5.1c Can we create a data set that enables research on identifying and quantifying hallucinated information in personalized abstractive summaries? Scope: medium-term, Related: Sections 2.4, 2.5  (S1210).</p>"
S380,"<p>TLDR users might need to verify information by comparing and contrasting multiple data sources (S1211). When developing and assessing models for inclusion in the TLDR, it will be necessary to have access to a data set that contains content about news and events from a variety of data sources and perspectives (S1212).</p>"
S381,"<p>Question 5.1d Can we create a data set that enables research to identify a set of local/regional news sources and articles which have greater detail, given a set of national/world news articles tagged as of interest to the user? Scope: medium-term, Related: Section 4.4  (S1213).</p>"
S382,<p>5.2 Data Set Augmentations (S1214).</p>
S383,"<p>Once we have a curated data set (or sets), in order to present a tailored subset of that data to the user it will be necessary to augment the data using a variety of methods (S1215). This could be by applying summarization to a collection of information within the data set, or by modifying the actual data available to a user, either based on their feedback or their ability to access specific data (S1216).</p>"
S384,"<p>A TLDR will employ a combination of models to identify relevant content and condense that content into a personalized concise summary for each user (S1217). It is necessary to understand the interaction between different models and the impacts of that interaction on various aspects of a TLDR, such as accuracy, relevance, and usability (S1218). To do so, we will need a data set that enables us to observe and measure such interactions and their impact on a TLDR (S1219).</p>"
S385,"<p>Question 5.2a Can we employ automatic summarization tools and techniques to generate headlines or short summaries that are compatible with recommender models? Scope: short-term, Related: Sections 3.1, 2, 4.4  (S1220).</p>"
S386,"<p>Question 5.2b How can we elicit user feedback regarding the quality and/or relevance of recommendations generated by creating headlines/short summaries through automatic summarization? Scope: medium-term, Related: Sections 3.2, 2, 4.4  (S1221).</p>"
S387,"<p>Question 5.2c How does the summarization technique employed to create a headline/short summary affect the quality and/or relevance of recommendations based on those headlines/short summaries? Scope: medium-term, Related: Sections 3.2, 2, 4.4  (S1222).</p>"
S388,"<p>Within a dataset, there may be data considered too sensitive for most users to see (S1223). It is possible, though, that the user may be allowed to see knowledge and/or recommendations drawn from that data (S1224). For example, an analyst could be shown the fact that a local militia is transporting weapons via a certain route, but not the fact that an informant in the group provided the information. (S1225).</p>"
S389,<p>Question 5.2d How do we handle segmented/protected information that specific users may not be allowed to access? Can we still use that data to generate recommendations? (S1226).</p>
S390,<p>5.3 Assessing quality of synthetic datasets (S1227).</p>
S391,"<p>In 2023, researchers at SCADS created a synthetic dataset using chatGPT and a digital version of the Zendian Problem by Lambros D (S1228). Callimahos about the fictional nation of Zendian (S1229). The dataset consists of 1000’s of synthetic intelligence reports about Zendia (S1230). These reports are available to SCADS participants in 2024. (S1231).</p>"
S392,"<p>Once we have a synthetically created dataset such as the Zendian dataset, we will need to have techniques for assessing the quality of that dataset (S1232). In this way we can be assured that research results generated from synthetic datasets will carry over to actual datasets (S1233). Typical techniques may have to be altered since the normal determination of hallucinations will not be valid, since the entire dataset is synthetic. (S1234).</p>"
S393,"<p>Question 5.3a Can we determine the extent to which a synthetic dataset of fictional intelligence reports is a quality proxy for an actual set of intelligence reports (S1235).  Scope: medium-term, Related: Section 5.1 (S1236).</p>"
S394,"<p>Question 5.3b Given a synthetic dataset, can we assess where the set is internally consistent with respect to its own timeline (S1237). Scope: medium-term, Related: Section 5.1 (S1238).</p>"
S395,"<p>While the Zendian dataset is useful for some research tasks, it does not currently contain enough user feedback data to be useful for creating recommender systems. (S1239).</p>"
S396,"<p>Question 5.3c Given a synthetic dataset, can we create synthetic user-item interaction data sufficient to enable recommender systems research (S1240). Scope: medium-term, Related: Section 5.1 (S1241).</p>"
S400,<p>6 Other Areas  (S1242).</p>
S401,"<p>Other areas of research will also be critical to the creation of TLDRs (S1243). Some of those areas are anticipated, but do not fit into one of the categories above (S1244). Some areas are unanticipated (S1245). We welcome projects in areas not stipulated here which will serve to further the eventual goal of creating Tailored Daily Reports for individual IC knowledge workers, and ask participants to coordinate with SCADS organizers to ensure such projects are aligned with SCADS objectives (S1246).</p>"
S402,"<p>While we plan to hold a classified post-conference workshop during SCADS 2024, it will focus on questions of infrastructure and specific use cases, not general research (S1247). We anticipate that all research involving the TLDR will remain completely unclassified in 2024 (S1248).</p>"
S404,<p>Appendix A (S1249). Quick Start Ideas  (S1250).</p>
S405,<p>We sketch an idea with the coding details in a notebook in the occams repository (S1251). The outline below gives improved ROUGE scores on several summarization datasets (S1252).</p>
S406,<p>Given training summarization training data with document(s)-summary(ies) pairs (S1253).</p>
S407,<p>Process the data as occams Document instances (S1254).</p>
S408,"<p>For each human summary sentence, find a sentence in the document(s) with the highest density of human summary terms for that sentence (S1255).</p>"
S409,"<p>Fit sentence classification model on the above data, as suggested below (S1256).</p>"
S410,<p>Use sentence embedding to generate vectors for the above-matched sentences (S1257).</p>
S411,<p>Apply canonical correlation analysis (CCA) to find a projection into lower dimensional space to maximize the correlations between the vectors (S1258).</p>
S412,<p>Train a classifier to distinguish between sentences that have dense overlap with human summary sentences (S1259).</p>
S413,<p>Now test the above model on held-out data  (S1260).</p>
S414,<p>Embed sentences from the document (S1261).</p>
S415,<p>Project them with the CCA model and use the classifier to estimate the probability the sentence has dense overlap with a summary sentence (S1262).</p>
S416,<p>Compute a matrix-vector product between the term-sentence matrix and the above sentence probabilities to compute term-weights for occams (S1263).</p>
S417,<p>Generate summaries for the test data with the computed term weights (S1264).</p>
S418,"<p>This experiment could be conducted by using a sample of real world cyber threat intelligence documents to fine-tune a number of different summarization machine learning models (S1265). To assess this approach, take the provided set of realistic intelligence questions, written by Elemendar’s lead analyst, and try to answer the questions with your own summarization tool and see if you can beat the benchmark scores set by our analysts and ChatGPT (S1266).</p>"
S420,<p>Appendix B (S1267). SAGA  (S1268).</p>
S421,"<p>Figure 1 gives an illustration where we are given five items a, b, c, d, and e, as well as pairwise similarity and dissimilarity scores (S1269). Note that each dissimilarity score is the complement of the corresponding similarity score (S1270). On the right side of the figure, the heatmap representation with the given ordering suggests some clustering of the items (S1271). Depending on the threshold chosen, one could infer perhaps two or three clusters (S1272).</p>"
S423,<p>Figure 1: Illustration of similarity and dissimilarity matrices (S1273).</p>
S424,"<p>Picking a single threshold value on the similarity score to use is a difficult task (S1274). SAGA, instead, considers all threshold choices (S1275). Figure 2 shows the graph the number of clusters as the similarity score threshold varies between 0 and 1 (S1276). Between 0 and 0.1, there are 5 clusters, between 0.1 and 0.2 there are 4 clusters, and so on (S1277). We denote the number of clusters as C(r), where r is the threshold value (S1278). The second plot in the Figure is a dendrogram illustrating the clusters as the threshold changes (S1279).</p>"
S425,<p>Graph of C(r)  (S1280).</p>
S427,<p>Figure 2: A graph of the number of clusters C(r) of similar objects under different similarity thresholds and a dendogram illustrating how clusters are combined as the threshold increases  (S1281).</p>
S429,<p>SAGA aggregates the information by defining an integral  (S1282).</p>
S430,<p>. (S1283).</p>
S431,"<p>Thus, I(X) gives the expected number of clusters in the set X with respect to the uniform distribution on the threshold r (S1284).</p>"
S432,"<p>Given two sets X and Y, we can then define a similarity score between them as follows: (S1285).</p>"
S434,<p>SAGA also defines a directional similarity between two sets by the formula:  (S1286).</p>
S436,<p>which is interpreted as the proportion of information in X and Y relative to the amount of information known in Y (S1287).</p>
S438,<p>Appendix C (S1288). Analyst Input on TLDR  (S1289).</p>
S439,"<p>The job of the intelligence analyst is multi-faceted, and the analyst workflow is complex (S1290). A day in the life of an analyst is a lot like a snowflake–no two are the same, but the underlying structures remain constant (S1291). To be successful, analysts must not only maintain situational awareness of current events, but must also discover and make sense of new information (S1292). As events around the world unfold, analysts must understand the ever-changing needs of the customer, and adjust the focus of their day to best answer key intelligence questions critical to national security (S1293). Because of the unique role of each analyst, a one-size-fits-all information bulletin does not work (S1294). Each analyst’s individual daily functions are highly dynamic and unique (S1295). Analysts must be constantly monitoring and adapting to changing priorities, triaging new raw information coming in, and adjusting their workflow as necessary to provide the highest value information as fast as possible (S1296). What’s needed is a platform that customizes the information analysts need to determine the focus of their work day, including information such as trends over the last 24 hours, important connections (or lack thereof), anomalies, current customer interest, etc (S1297).</p>"
S440,"<p>Analysts are primarily responsible for analyzing the raw data from one type of intelligence aka “INT.” For example, analysts from NSA would focus on signals intelligence or SIGINT (S1298). Since analysts are part of a bigger intelligence community, it’s important for analysts to read and incorporate the published information from other “INTs” (human intelligence (HUMINT), measurements intelligence (MASINT), open source intelligence (OSINT), etc.) into their analysis, strategies, and workflows (S1299).</p>"
S441,<p>Often analysts don’t have time to search for and thoroughly read other types of intelligence (S1300). Being able to combine information and reporting from multiple intelligence types in one place–ideally one cohesive narrative rather than separate summaries for each “INT”–would be beneficial (S1301).</p>
S442,"<p>It’s not enough to have information from these different “INTs” (S1302). Incorporating open source information from regional news/data sources provides context to intelligence, and can confirm or deny a fact gleaned from intelligence sources (S1303). However, care must be taken to not overwhelm analysts with too much information (S1304). Having a platform with the capability of allowing analysts to adjust the level of information, as well as the format they would like that information to be presented in, would be ideal (S1305).</p>"
S443,"<p>Having a global mission means there are vast amounts of documented data, and there are many people, places, and things that are of potential interest to analysts (S1306). However, analysts don’t have time to thoroughly analyze them, and discover and understand the relationships and networks among them (S1307). Analysts need to be able access this knowledge, add more data, and make sense of it (S1308). To further increase efficiency, affording analysts the ability to seamlessly perform follow-on queries in their investigations would be highly beneficial (S1309). If analysts were able to track their lines of inquiry by adding notes to information they encounter, it would not only save time, but allow analysts to track where thinking may have gone off course when looking retrospectively (S1310).</p>"
S444,"<p>Intelligence production is a team sport, so it’s vital for analysts to have some insight into what other analysts are doing and how they are doing it (S1311). Analysts work together on missions and targets, and transparency into the workflows of their colleagues is essential for coordination and deconfliction (S1312). Likewise, analytic tools and tradecraft are constantly changing and analysts don’t always have the time to search for new and better ways of doing their work (S1313). Having access to the latest tradecraft and tools relevant to their analytic questions integrated into their workflows would save time, and potentially improve performance (S1314).</p>"
S445,"<p>Relatedly, analysts have access to an overwhelming number of tools, capabilities, and datasets (S1315). It is not realistic to expect them to be experts, or even familiar with, each one of them (S1316). Sometimes analysts don’t have time to learn a particular tool, or it’s just one they struggle with, so they are left with three options: 1) make time to learn the tool; 2) ask a proficient peer to use the tool/capability on their behalf, or 3) forgo the task (S1317). Analysts would benefit from a capability that could learn the level of proficiency of the analyst, assess weaknesses, and provide affordances (S1318).</p>"
S446,"<p>In summary, analysts would benefit from a TDLR that allows them to:  (S1319).</p>"
S447,<p>Maintain awareness of reporting across all INT types  (S1320).</p>
S448,<p>Adjust and calibrate the level of detail and information analyst take in from various sources (S1321).</p>
S449,"<p>Quickly access, annotate, and generate follow-on queries from new information received  (S1322).</p>"
S450,<p>View and track tasks and tradecraft across their team and other mission collaborators  (S1323).</p>
S451,"<p>Directly access tools and relevant tradecraft, with affordances provided for each analyst’s strengths and weaknesses (S1324).</p>"

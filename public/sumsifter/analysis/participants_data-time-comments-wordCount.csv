Participant ID,Prolific ID,Completed,Total Time (seconds),Total Word Count,introduction_0_response,introduction_0_time (seconds),introduction_0_word_count,tutorial_1_response,tutorial_1_time (seconds),tutorial_1_word_count,sumsift1_2_response,sumsift1_2_time (seconds),sumsift1_2_word_count,sumsift2_3_response,sumsift2_3_time (seconds),sumsift2_3_word_count,sumsift4_4_response,sumsift4_4_time (seconds),sumsift4_4_word_count,sumsift5_5_response,sumsift5_5_time (seconds),sumsift5_5_word_count,sumsift3_6_response,sumsift3_6_time (seconds),sumsift3_6_word_count,feedback-tutorial_7_response,feedback-tutorial_7_time (seconds),feedback-tutorial_7_word_count,feedback-process-1_8_response,feedback-process-1_8_time (seconds),feedback-process-1_8_word_count,feedback-process-2_9_response,feedback-process-2_9_time (seconds),feedback-process-2_9_word_count,feedback-process-3_10_response,feedback-process-3_10_time (seconds),feedback-process-3_10_word_count,survey_11_response,survey_11_time (seconds),survey_11_word_count,Total Time (seconds),Total Word Count,sumsift5_2_response,sumsift5_2_time (seconds),sumsift5_2_word_count,sumsift3_3_response,sumsift3_3_time (seconds),sumsift3_3_word_count,sumsift2_4_response,sumsift2_4_time (seconds),sumsift2_4_word_count,sumsift4_5_response,sumsift4_5_time (seconds),sumsift4_5_word_count,sumsift1_6_response,sumsift1_6_time (seconds),sumsift1_6_word_count,sumsift4_2_response,sumsift4_2_time (seconds),sumsift4_2_word_count,sumsift4_3_response,sumsift4_3_time (seconds),sumsift4_3_word_count,sumsift3_4_response,sumsift3_4_time (seconds),sumsift3_4_word_count,sumsift2_5_response,sumsift2_5_time (seconds),sumsift2_5_word_count,sumsift5_6_response,sumsift5_6_time (seconds),sumsift5_6_word_count,sumsift5_3_response,sumsift5_3_time (seconds),sumsift5_3_word_count,sumsift3_5_response,sumsift3_5_time (seconds),sumsift3_5_word_count,sumsift4_6_response,sumsift4_6_time (seconds),sumsift4_6_word_count,sumsift1_4_response,sumsift1_4_time (seconds),sumsift1_4_word_count,sumsift2_6_response,sumsift2_6_time (seconds),sumsift2_6_word_count,sumsift3_2_response,sumsift3_2_time (seconds),sumsift3_2_word_count,consent_1_response,consent_1_time (seconds),consent_1_word_count,tutorial_2_response,tutorial_2_time (seconds),tutorial_2_word_count,rectangleBrush_q1_3_response,rectangleBrush_q1_3_time (seconds),rectangleBrush_q1_3_word_count,sumsift4_7_response,sumsift4_7_time (seconds),sumsift4_7_word_count,sumsift5_8_response,sumsift5_8_time (seconds),sumsift5_8_word_count,feedback-process-1_9_response,feedback-process-1_9_time (seconds),feedback-process-1_9_word_count,feedback-process-2_10_response,feedback-process-2_10_time (seconds),feedback-process-2_10_word_count,feedback-process-3_11_response,feedback-process-3_11_time (seconds),feedback-process-3_11_word_count,post-study-survey_12_response,post-study-survey_12_time (seconds),post-study-survey_12_word_count,survey_13_response,survey_13_time (seconds),survey_13_word_count,sumsift1_5_response,sumsift1_5_time (seconds),sumsift1_5_word_count,sumsift5_4_response,sumsift5_4_time (seconds),sumsift5_4_word_count
6a27316d-7d38-459b-be74-47b32ce2146b,KW,True,3387.063,548,,19.021,0,,51.192,0,"I'm just going to dump my thoughts with no particular order here:
1. It not very clear to me what the numbering means for each source. 
2. When there are multiple sources assigned to one sentence, it can be challenging to understand which source contribute to the exact portion of the summary, then it leads to longer time to read through the actual document. 
3. Some attributed sources are not correct, for example S15, for the second to the last sentence, not sure if this is intentional. 
4. I'm kind of curious to see at what point an analyst will click on the source and verify it themselves. If they need to click through every one of them, it's too much effort. This probably has to do with the data that's used here. Since I'm familiar with the SCADS document, so reading through the summary is quite easy and I don't need to verify their sources. But if it's a very technical article in chemistry, for example, then I have this natural suspicion that makes me want to probably click everything to see the source...
5. Also, there's no time constraint for me when interacting with this, but will people pay less attention to the sources if they are under pressure?
6. Very tiny point, if this feedback box is a bit bigger will be very helpful.",935.831,227,,,,,,,,,,,,,,14.478,0,,302.669,0,,123.082,0,"The first thing that came to my mind when I looked at the interface is the color-coding of the search parameters. Is there any particular meaning? I was initially associating the D2, D5, and D9 in green to ""APT X"" as those articles are only based this one parameter indicated by the color. 
Beyond this, I like the overall flow from one panel to the next with more fine-grained information displayed. ",409.942,71,,126.819,0,3387.063,548,,,,,,,,,,,,,,,,,,,,,,"This is just a random thought. Now, every sentence in the summary has labeled sources. What if analysts have the choice to click on the sentence and request a source? I'm thinking maybe that way, visually it's less crowded for analysts to read and you can see what type of sentences triggers analysts to seek out original sources.
2. Including sources for every sentences may give analysts false confidence after they click through a few and verify them to be correct, will they later overthrust the summary? ",445.854,87,"I don't think I can add anything specific here... But a general thought when I interact with summarization is that I can never be sure the summarization actually covers all the information mentioned in the original document. This is not about the details in the original doc that got ignored, but more like a whole chunk of a section is not covered in the summary kind of situation.",397.01,68,,,,"1. So the length of the summarization can also be changed. Not sure if this will affect people's choice on reviewing the source since they don't need to click through many.
2. How many sources can one sentence attributed to? 
3. Many sources when I clicked only led to a single sentence (like S26, S54, S72), then it makes me doubt how well this summary does. I start wanting to know why that three sentences are the most important to include in the summary...",513.416,84,,,,I personally think one source per sentence is easier to verify. ,47.749,11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9413370c-2953-45ee-9c5b-41b16f5e6b7d,Derek,True,3580.615,185,,32.105,0,,35.865,0,,,,,,,,,,I think this is a good high-level summary of SCADs in general. I do think that it should include more references to sections where the reader can learn more about each topic if they choose too.,295.171,36,,,,,7.592,0,,29.796,0,,37.353,0,"Overall, I like this design. I think the only thing I would change is move the ""Technical Details and Implementation"" section to the summary page. I think the technical details may not need to be displayed right away.",329.468,38,,374.317,0,3580.615,185,,,,,,,,256.672,0,,,,"I think this summary starts off fine, then in the middle starts listing things, then goes back to summarizing.

This probably won't happen too often, but if you swap between full screen and windowed the marker/reference line disconnects and won't fix itself until you click a new reference.",1011.245,48,,,,"This summary has a similar listing problem, but it also mentions things out of order, jumps around, or repeats information multiple times. It also seems to change the tone or leave out the source in some places. For example, the rephrasing of S17 doesn't seem to match the same tone/intent as the source and S606 does include the line of that mentions bias.",498.948,63,,,,,,,,,,,,,,,,,,,,,,,,,,672.083,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f01eea9b-34b7-40b4-a2db-08b4ad10871a,Ed,True,8821.391,832,,63.714,0,,21.808,0,"I really like having the links back to the original source data showing attribution.  It does allow me to consider how accurate the summary sentence with regards to the source. For example, the sentance ""Techniques like VideoGPT..."" referenced S54 which does not appear to be accurately reflected in the summary.

I do think that the links feel a bit distracting to overall readability of the summary.  Maybe something like a hover over to expand or some other way of hiding the source data when reading the summary would make it overall easier to read.

I'm curious if there's a better way to actually summarize such a long document.  It almost feels like I'm missing a lot of information and, at the same time, not confident that the summary captures the key points accurately. I wonder if some way of segmenting the document before summarization (maybe then summarize that section) would improve understanding of the source.

I have no way to tell what type of information was not captured in the summary. Having the sources allows me to verify information (enhancing trust). At the same time, having the source document visible in full makes me question how many key points were missing from the summary.

Assuming a case with some retrieval step used before generating the summary, it would be interested to think about how to display some insight into which sections were retrieved/why something was retrieved. Not sure how to visualize that, but it would also help to understand which information may not have been considered (or could contradict something stated in the summary).

The answer/feedback box is way too small!!

I'm sure I'll have more random thoughts on further use and would really would like to try the tool again.",1599.756,291,"As soon as I see one inconsistency (inaccurate summary sentence), I don't feel that I can really trust the entire summary. This is interesting since I feel that the sourcing helps enhance trust, but makes it worse once ""broken"" with inaccurate information.",376.316,42,,,,,,,,,,,4537.35,0,,95.024,0,,54.669,0,"From a natural language and summarization aspect, I really like the flow and layout.  It seems to provide both relevant information to the query while allowing an analyst to ""dig in"" to the details when necessary or applicable. 

From an overall efficiency perspective, I wonder if there are more effective ways to display the initial summary.  For example, how similar will every summary end up (ex. ""D2 provides an overview..."" ""D5 delves into..."")? Maybe there are some form of outline-like summary that might make an analysts job more efficient.
When trying to quickly digest and determine applicability, it may be better to just plop the details in front of me instead of making me read some elegant summary. While it might read easier, it also may just take extra time without providing any actual value to the reader.

D3 and D7 are not even referenced in the overall relevance section. If you are grouping them already into sections, maybe move the group relevance to the actual section of those documents (ie. Documents D2, D5 and D9..."" could be moved to the section containing those docs while D3 and D7 would have its own intro on what that group means).

I wonder if there is anything that could be said about differences between grouped results such as what is not covered in one set but is covered in another set. Similarly within documents in a group. This would allow an analyst to see what was unique of different between individual documents.

Is there a way to update the search parameters or relevance scoring based on the list of results/document summary/source? For example, could I highlight a section in the source document or summary of a document to add it into the original search parameters and/or refine the summary with more info like the selected text. (Or, maybe some sort of ""focus on this"" option that could be adjusted as the analyst learns more about the dataset itself).",1190.835,326,,71.622,0,8821.391,832,,,,,,,,,,"I really like this summary better that the others until the very end where it uses a reference and some questions to describe the actual conference work.

This feels like is missing some of the necessary data as the sources only span half way through the document.  Maybe consider how to visualize where or how much of the document was actually used for the summary.  (Was the context window too small? Did you only retrive part of the document? Was there some other reason for not considering the other sections of the document?)",260.099,93,,,,,,,,,,"Interesting to see that the summary mostly grabbed sources from the very beginning and very end with little in between. 

Clearly the references are not accurate to the summary sentence. 

Easy to see in many cases, but obvious things such as a very specific word/term/phrase (ex. occams package) not contained in the reference makes it even worse. ",490.283,57,,,,I think that the type of summary will really depend on why I'm looking at a summary and what the purpose or user.,59.915,23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a14fe505-c93b-4be7-b58d-bb918c05b74d,Daria,True,4907.248,157,,22.088,0,,1.355,0,"The last sentence in the summary claims : ""The program mandates bi-weekly evaluations to ensure research quality"" which I believe is not a part of the original summary. First sentence seems to be not too condensed. It's quite impressive that LLM finds several mentions of the same term (RAG, as referred to S72, S118 in the summary) which is put together. However, some information seems to be misrepresented as the original document says RAG does not usually incorporate temporality.",519.648,79,,,,,,,I wonder what were the criteria for choosing the most important information?,48.787,12,,,,,4.296,0,,4.455,0,,1.894,0,I wonder if there is a way to keep track of those source documents and if analysts need to access them later. ,1072.378,22,,3.145,0,4907.248,157,,,,There seems to be no mention of the S1294. It would be nice to have control over the length of the summary and level of detail.,3070.602,26,Is there more than one Grand Challenge? A very very condensed summary in ratio to the source document.,93.278,18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,65.322,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e38cb954-fa12-4caf-b250-d1820ba2b0bc,RJC,True,754.034,124,,6.801,0,,4.014,0,"The interface is really slick! 

Some of the summarized sentences are semi-hallucinatory at least in their implication (e.g. ""...combining classified and unclassified information..."")

The formatting could probably stand a few line breaks, esp. in the source document (but this is minor).",282.801,41,"Ooh, same semi-hallucination!",26.52,3,,,,,,,,,,,5.393,0,,23.926,0,,13.996,0,Could we incorporate a dis-function style feedback mechanism that allows people to select/deselect tags on the initial set of relevant documents to make the process more elastic?,97.979,27,,6.113,0,754.034,124,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Bug: S51 doesn't highlight anything in the course text.

Aside: I would love to see an extractive summary built from the source sentences in the same order that they're references in the LLM-generated summary.",116.092,34,"Overall, this summary feels more fragmented.

Semi-hallucination: S118 doesn't exactly imply ""Temporal elements in RAG pipelines ensure up-to-date information""",114.032,19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,56.367,0
d8a6b9dc-9836-4344-8567-1a2b528456ef,Keltin Grimes,True,1201.762,167,,11.711,0,,20.135,0,,,,,,,,,,,,,,,,,6.621,0,,221.404,0,,21.431,0,Seems reasonable,98.986,2,,4.519,0,1201.762,167,,,,,,,,,,,,,,215.248,0,,,,It's interesting that there's only one citation per sentence on this one. It seems like the sentence are shorter and more concise (presumably to only describe one reference at a time). I can see it being good or bad depending on use case.,148.719,43,,,,"The double citation ""introduced (S206) [S206]"" is an interesting artifact. The last couple sentences are totally bogus (maybe I should check the other ones too).",110.567,25,,,,,,,,,,,,,,,,,,,Might be nice to have tooltips where you can hover over a citation and the reference sentence would pop up there. Also would be interesting to go the other way: click on a sentence in the reference document and see what parts of the summary are attributed to that. Would be useful if there's a sentence that the user thinks is very important and wants to check if the summary refers to it. Also the summary here doesn't cite the sentence where they first introduce TLDRs which is interesting.,321.52,89,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"If you want something short, this seems good.",20.901,8
d53e0f77-ec42-46ea-9018-6eb1b2744eab,Ashley ,True,1088.805,321,,32.997,0,,23.91,0,,,,I really like that the selection jumps to the full text section when I click a source link. Might be interesting to have some kind of visual overview of the text so that I can get a sense of how long the source text is without scrolling. Also how much of the source text made it to the summary. For instance? Is the summary pulling equally from every section or does it tend to include portions from the beginning and end?,187.925,81,"For the longer summaries, the source links break up the text a lot more and was more difficult for me to read. I wonder if they could be more compact or if the blue source links could be a something you toggle on, like a secondary step? ",164.717,47,,,,"With this one and the last one, I found the summary easier to read with the source links were grouped like citations. could make sense to make source links mimic the style of citation familiar too analysts? Or maybe not. ",93.75,40,,5.286,0,,26.396,0,,38.922,0,"Great use of nesting for provenance. Love being able to see the the papers [d2, d5, d9] bundled together. Makes me wonder if you could toggle on a selection of documents from those. For example, could I opt to just see a summary of D2 and D9? ",183.878,47,,28.995,0,1088.805,321,"For some reason, when I click the source links to see where the summarized text comes from, I want to see the summarized portion highlighted too so that I can compare if I want.",130.034,34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"I feel like this is a sweet spot for me in terms of the summary. It was enough information to get somewhat specific without being too much. Though I could understand wanting a shorter version. It could be interesting to let the user determine how ling they want a summary to be. Maybe they start as short as possible, but decide they want to see what a longer summary would look like?",171.995,72,,,
65b1367c-5d89-4fcf-8562-afa3874e6a70,Melissa is the coolest Evans,True,1455.139,444,,24.961,0,,27.18,0,,,,,,,,,,,,,,,,,1.983,0,,26.938,0,,7.881,0,"Love it, again, I think the summary of a document needs to begin with a high level overview of what the document is though. 

Also not sure if this is super clear because you explained the concept to me previously, for others it might help to have text at the top saying ""if you click here, this opens up"". 

Not clear form this visualization if all of this would be visible at once or if this is showing how a screen would change. ",104.074,83,,13.608,0,1455.139,444,,,,,,,"I like the length of this one! - weird hiccup looks like it says (S206) before the link to S206.
Also not a huge fan of S100 - it lists a specific statistic and then links to a sentence with no statistic. ",185.204,42,,,,,,,,,,,,,,,,,,,,,,"I think the last one may have been too long, but this one may be too short for trying to give us enough info about the full document. One way to land in the middle could be to list things out as bullet points or to add paragraph breaks to a longer summary. ",178.212,53,,,,"One thought (that may be irrelevant to this project, apologies if so) is that something analysts seem to want help with is quickly identifying what documents are relevant or not. I think it would help if a summary begins with like two sentences describing what the document is (e.g., ""this text described the goals and ambitions of a 8 week conference program called SCADS. It is penned by X and intended for Y audience"").

For this summary - seems like the last but more balanced on the problem areas. ",182.166,89,,,,,,,"It's not clear to me what the different S# buttons mean - is it sentence 251? if so, I'm not sure you need s251, etc. for the analyst, they'll see when they click how far down in the document it is. Just using numbers 1-100 based on the order the source comes up in the document might be helpful as it would remind analysts of citation patterns in academic articles. 
Another thought - sometimes theres four sources listed but then they're all sequential sentences. Would be nice if it was just one link that lead to a highlighted paragraph instead of having to click through each sentence to see they're all together.

Also might be helpful if there are multiple sources for a single line if they're listed sequentially (the order they come in the full text). As people click through these sources they get a feel for the full document, and keeping things chronological would help with that. ",626.114,159,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,There's a slightly odd emphasis on summarization while recommendation only gets one sentence. Otherwise I like this one!,76.818,18,,,
3a6dc890-468b-49e3-8a38-da954a03c773,Ben S,True,588.022,136,,21.006,0,,18.622,0,"Nice UI, I would suggest reducing the source document to the size of the summary, and the summary even further to half it's current size if you expect participants to fully engage with the whole source document, some of the links appear incorrect e.g. s256 points to a bibliography reference?",198.83,50,,,,,11.737,0,Will there be questions to try and answer using only the summary?,58.935,12,,,,,6.439,0,,47.127,0,,10.647,0,"I think i understand what is going on, but maybe a non CTI person would like some sort of upfront objective e.g. you are trying to find out this group's motivations etc?",131.815,32,,7.444,0,588.022,136,,,,,14.622,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"If the intention is to present people with 10 summaries in a row and ask them to rate them, I think people might stuggle to stay engaged, it can be difficult to maintain focus when reading the same thing re-written several times
",60.798,42,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
